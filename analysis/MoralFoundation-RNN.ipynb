{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "import gensim\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "spw_set = set(stopwords.words('english'))\n",
    "spw_set.add('url')\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>text</th>\n",
       "      <th>corpus</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>521033092132503552</td>\n",
       "      <td>@fergusonoctober @fox2now #alllivesmatter peac...</td>\n",
       "      <td>ALM</td>\n",
       "      <td>care,purity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537681598989475841</td>\n",
       "      <td>wholeheartedly support these protests acts of ...</td>\n",
       "      <td>ALM</td>\n",
       "      <td>subversion,loyalty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>624644420705648640</td>\n",
       "      <td>this sandra bland situation man no disrespect ...</td>\n",
       "      <td>ALM</td>\n",
       "      <td>cheating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>752979765984890884</td>\n",
       "      <td>commitment to peace, healing and loving neighb...</td>\n",
       "      <td>ALM</td>\n",
       "      <td>care,purity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>548029362348765185</td>\n",
       "      <td>injustice for one is an injustice for all #all...</td>\n",
       "      <td>ALM</td>\n",
       "      <td>cheating,loyalty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tid                                               text  \\\n",
       "0  521033092132503552  @fergusonoctober @fox2now #alllivesmatter peac...   \n",
       "1  537681598989475841  wholeheartedly support these protests acts of ...   \n",
       "2  624644420705648640  this sandra bland situation man no disrespect ...   \n",
       "3  752979765984890884  commitment to peace, healing and loving neighb...   \n",
       "4  548029362348765185  injustice for one is an injustice for all #all...   \n",
       "\n",
       "  corpus              labels  \n",
       "0    ALM         care,purity  \n",
       "1    ALM  subversion,loyalty  \n",
       "2    ALM            cheating  \n",
       "3    ALM         care,purity  \n",
       "4    ALM    cheating,loyalty  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('../data/dataset.tsv', sep='\\t', dtype=str)\n",
    "all_data.tid = all_data.tid.apply(lambda x: str(x))\n",
    "all_data = all_data[~all_data.text.isna()]\n",
    "all_data = all_data[~all_data.labels.isna()]\n",
    "print(len(all_data))\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing url\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess a single tweet\n",
    "    :param tweet:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # replace url\n",
    "    tweet = re.sub(r\"https?:\\S+\", \"URL\", tweet)\n",
    "    # replace user\n",
    "#     tweet = re.sub(r'@\\w+', 'USER', tweet)\n",
    "    # replace hashtag\n",
    "#     tweet = re.sub(r'#\\S+', 'HASHTAG', tweet)\n",
    "    # tokenize\n",
    "    return [item.strip() for item in tokenizer.tokenize(tweet) if len(item.strip())>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30979\n"
     ]
    }
   ],
   "source": [
    "# preprocess tweet and remove short tweet\n",
    "all_data.text = all_data.text.apply(lambda x: preprocess(x))\n",
    "all_data = all_data[all_data.text.apply(lambda x: len(x) > 3)]\n",
    "all_data.text = all_data.text.apply(lambda x: ' '.join(x))\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(raw_label):\n",
    "    all_labels = [\n",
    "        'subversion', 'loyalty', 'care', 'cheating',\n",
    "        'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "    ]\n",
    "    encode_label = [0]*len(all_labels)\n",
    "    if type(raw_label) != str:\n",
    "        encode_label[-1] = 1\n",
    "        return encode_label\n",
    "    for label in raw_label.split(','):\n",
    "        if label not in all_labels:\n",
    "            encode_label[-1] = 1\n",
    "        else:\n",
    "            encode_label[all_labels.index(label)] = 1\n",
    "    return encode_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.labels = all_data.labels.apply(lambda x: label_encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_average(y_preds, y_truths):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for idx, (y_pred, y_truth) in enumerate(zip(y_preds, y_truths)):\n",
    "        true_positives = np.sum(np.logical_and(y_truth, y_pred))\n",
    "\n",
    "        # compute the sum of tp + fp across training examples and labels\n",
    "        l_prec_den = np.sum(y_pred)\n",
    "        if l_prec_den != 0:\n",
    "            # compute micro-averaged precision\n",
    "            precisions.append(true_positives/l_prec_den)\n",
    "        \n",
    "        # compute sum of tp + fn across training examples and labels\n",
    "        l_recall_den = np.sum(y_truth)\n",
    "\n",
    "        # compute mirco-average recall\n",
    "        if l_recall_den != 0:\n",
    "            recalls.append(true_positives/l_recall_den)\n",
    "\n",
    "    precisions = np.mean(precisions)\n",
    "    recalls = np.mean(recalls)\n",
    "    if precisions + recalls == 0:\n",
    "        return 0\n",
    "    f1 = 2*precisions*recalls / (precisions + recalls)\n",
    "    return f1\n",
    "def multi_label_f1(y_preds, y_truths, mode='weighted'):\n",
    "    preds = dict()\n",
    "    truths = dict()\n",
    "    for idx in range(len(y_truths)):\n",
    "        for jdx in range(len(y_truths[idx])):\n",
    "            if jdx not in preds:\n",
    "                preds[jdx] = []\n",
    "                truths[jdx] = []\n",
    "            preds[jdx].append(y_preds[idx][jdx])\n",
    "            truths[jdx].append(y_truths[idx][jdx])\n",
    "    results = []\n",
    "    for jdx in preds:\n",
    "        results.append(metrics.f1_score(preds[jdx], truths[jdx], average=mode))\n",
    "    return np.average(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# load the vaccine data and test the classifier on the vaccine data\n",
    "vaccine_df = pd.read_csv('../data/vaccine_morality.csv', dtype=str)\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: preprocess(x))\n",
    "# vaccine_df = vaccine_df[vaccine_df.text.apply(lambda x: len(x) > 3)]\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: ' '.join(x))\n",
    "vaccine_df = vaccine_df.sample(frac=1).reset_index(drop=True)\n",
    "print(len(vaccine_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_data = {\n",
    "    'train_x': [],\n",
    "    'train_y': [],\n",
    "}\n",
    "\n",
    "all_labels = [\n",
    "    'subversion', 'loyalty', 'care', 'cheating',\n",
    "    'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "]\n",
    "\n",
    "for idx, row in vaccine_df.iterrows():\n",
    "    encode_label = [0] * len(all_labels)\n",
    "    for label_index, label in enumerate(all_labels):\n",
    "        if np.isnan(np.array(row[all_labels[label_index]], dtype=np.float32)):\n",
    "            continue\n",
    "        if int(row[all_labels[label_index]]) == 1:\n",
    "            encode_label[label_index] = 1\n",
    "    if sum(encode_label) == 0:\n",
    "        encode_label[-1] = 1\n",
    "    vaccine_data['train_x'].append(row['text'])\n",
    "    vaccine_data['train_y'].append(encode_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just got a covid vaccine shout out to fatness :)', '@shurrell1 @badcovid19takes no , they believe the vaccine is deadlier than covid .', '@wcvb why would you take a vaccine for covid when the recovery rate is 99.9 % using therapeutics ? ðŸ‡º ðŸ‡¸ URL', 'just hoping covid gets it â€™ s ass kicked as hard as mine did ice skating w / this new vaccine #newprofilepic URL', 'i got my first covid vaccine ! crying tears of joy in the vaccination area , feeling especially proud and grateful for science today ! ! !']\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vaccine_data['train_x'][:5])\n",
    "print(vaccine_data['train_y'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode domains\n",
    "domain_encoder = list(all_data.corpus.unique()) + ['vaccine']\n",
    "\n",
    "# use half of the vaccine as train and half as test\n",
    "all_train = {\n",
    "    'docs': all_data.text.to_list() + vaccine_data['train_x'][:250],\n",
    "    'labels': all_data.labels.to_list() + vaccine_data['train_y'][:250],\n",
    "    'corpus': all_data.corpus.to_list() + ['vaccine'] * 250\n",
    "}\n",
    "all_train['corpus'] = [domain_encoder.index(item) for item in all_train['corpus']]\n",
    "vaccine_train = {\n",
    "    'docs': vaccine_data['train_x'][:250],\n",
    "    'labels': vaccine_data['train_y'][:250],\n",
    "    'corpus':  [len(domain_encoder)-1] * 250\n",
    "}\n",
    "vaccine_test = {\n",
    "    'docs': vaccine_data['train_x'][250:],\n",
    "    'labels': vaccine_data['train_y'][250:],\n",
    "    'corpus':  [len(domain_encoder)-1] * 250\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wt(tkn, emb_path, opath):\n",
    "    \"\"\"Build weight using word embedding\"\"\"\n",
    "    embed_len = len(tkn.word_index)\n",
    "    if embed_len > tkn.num_words:\n",
    "        embed_len = tkn.num_words\n",
    "\n",
    "    if emb_path.endswith('.bin'):\n",
    "        embeds = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            emb_path, binary=True, unicode_errors='ignore'\n",
    "        )\n",
    "        emb_size = embeds.vector_size\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        for pair in zip(embeds.wv.index2word, embeds.wv.syn0):\n",
    "            if pair[0] in tkn.word_index and \\\n",
    "                    tkn.word_index[pair[0]] < tkn.num_words:\n",
    "                emb_matrix[tkn.word_index[pair[0]]] = np.asarray([\n",
    "                    float(item) for item in pair[1]\n",
    "                ], dtype=np.float32)\n",
    "    else:\n",
    "        dfile = open(emb_path)\n",
    "        line = dfile.readline().strip().split()\n",
    "        if len(line) < 5:\n",
    "            line = dfile.readline().strip().split()\n",
    "        emb_size = len(line[1:])\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        dfile.close()\n",
    "\n",
    "        with open(emb_path) as dfile:\n",
    "            for line in dfile:\n",
    "                line = line.strip().split()\n",
    "                if line[0] in tkn.word_index and \\\n",
    "                        tkn.word_index[line[0]] < tkn.num_words:\n",
    "                    emb_matrix[tkn.word_index[line[0]]] = np.asarray([\n",
    "                        float(item) for item in line[1:]\n",
    "                    ], dtype=np.float32)\n",
    "    # emb_matrix = np.array(emb_matrix, dtype=np.float32)\n",
    "    np.save(opath, emb_matrix)\n",
    "    return emb_matrix\n",
    "\n",
    "\n",
    "def build_tok(docs, max_feature, opath):\n",
    "    if os.path.exists(opath):\n",
    "        return pickle.load(open(opath, 'rb'))\n",
    "    else:\n",
    "        # load corpus\n",
    "        tkn = Tokenizer(num_words=max_feature)\n",
    "        tkn.fit_on_texts(docs)\n",
    "\n",
    "        with open(opath, 'wb') as wfile:\n",
    "            pickle.dump(tkn, wfile)\n",
    "        return tkn\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, dataset, domain_name):\n",
    "        self.dataset = dataset\n",
    "        self.domain_name = domain_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['docs'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.domain_name in self.dataset:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], self.dataset[self.domain_name][idx]\n",
    "        else:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularRNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RegularRNN, self).__init__()\n",
    "        self.params = params\n",
    "\n",
    "        if 'word_emb_path' in self.params and os.path.exists(self.params['word_emb_path']):\n",
    "            self.wemb = nn.Embedding.from_pretrained(\n",
    "                torch.FloatTensor(np.load(\n",
    "                    self.params['word_emb_path'], allow_pickle=True))\n",
    "            )\n",
    "        else:\n",
    "            self.wemb = nn.Embedding(\n",
    "                self.params['max_feature'], self.params['emb_dim']\n",
    "            )\n",
    "            self.wemb.reset_parameters()\n",
    "            nn.init.kaiming_uniform_(self.wemb.weight, a=np.sqrt(5))\n",
    "\n",
    "        if self.params['bidirectional']:\n",
    "            self.word_hidden_size = self.params['emb_dim'] // 2\n",
    "        else:\n",
    "            self.word_hidden_size = self.params['emb_dim']\n",
    "\n",
    "        # domain adaptation\n",
    "        self.doc_net_general = nn.GRU(\n",
    "            self.wemb.embedding_dim, self.word_hidden_size,\n",
    "            bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        # prediction\n",
    "        self.predictor = nn.Linear(\n",
    "            self.params['emb_dim'], self.params['num_label'])\n",
    "\n",
    "    def forward(self, input_docs):\n",
    "        # encode the document from different perspectives\n",
    "        doc_embs = self.wemb(input_docs)\n",
    "        _, doc_general = self.doc_net_general(doc_embs)  # omit hidden vectors\n",
    "\n",
    "        # concatenate hidden state\n",
    "        if self.params['bidirectional']:\n",
    "            doc_general = torch.cat((doc_general[0, :, :], doc_general[1, :, :]), -1)\n",
    "\n",
    "        if doc_general.shape[0] == 1:\n",
    "            doc_general = doc_general.squeeze(dim=0)\n",
    "\n",
    "        # prediction\n",
    "        doc_preds = self.predictor(doc_general)\n",
    "        return doc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder(object):\n",
    "    def __init__(self, params, mtype='rnn'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param params:\n",
    "        :param mtype: Model type, rnn or bert\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.mtype = mtype\n",
    "        if self.mtype == 'rnn':\n",
    "            self.tok = pickle.load(open(\n",
    "                os.path.join(params['tok_dir'], '{}.tok'.format(params['dname'])), 'rb'))\n",
    "        elif self.mtype == 'bert':\n",
    "            self.tok = BertTokenizer.from_pretrained(params['bert_name'])\n",
    "        else:\n",
    "            raise ValueError('Only support BERT and RNN data encoders')\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        docs = []\n",
    "        labels = []\n",
    "        domains = []\n",
    "        for text, label, domain in batch:\n",
    "            if self.mtype == 'bert':\n",
    "                text = self.tok.encode_plus(\n",
    "                    text, padding='max_length', max_length=self.params['max_len'],\n",
    "                    return_tensors='pt', return_token_type_ids=False,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                docs.append(text['input_ids'][0])\n",
    "            else:\n",
    "                docs.append(text)\n",
    "            labels.append(label)\n",
    "            domains.append(domain)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        domains = torch.tensor(domains, dtype=torch.long)\n",
    "        if self.mtype == 'rnn':\n",
    "            # padding and tokenize\n",
    "            docs = self.tok.texts_to_sequences(docs)\n",
    "            docs = pad_sequences(docs)\n",
    "            docs = torch.Tensor(docs).long()\n",
    "        else:\n",
    "            docs = torch.stack(docs).long()\n",
    "        return docs, labels, domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = '../resource/results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "model_dir = '../resource/model/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "model_dir = model_dir + 'vaccine/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "        \n",
    "params = {\n",
    "    'result_path': os.path.join(result_dir, 'vaccine.txt'),\n",
    "    'model_dir': model_dir,\n",
    "    'dname': 'vaccine',\n",
    "    'max_feature': 15000,\n",
    "    'over_sample': True,\n",
    "    'domain_name': 'corpus',\n",
    "    'epochs': 15,\n",
    "    'batch_size': 64,\n",
    "    'lr': 9e-5,\n",
    "    'max_len': 100,\n",
    "    'dp_rate': .2,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'emb_path': '/data/models/glove.twitter.27B.200d.txt',  # adjust for different languages\n",
    "    'emb_dim': 200,\n",
    "    'unique_domains': [],\n",
    "    'bidirectional': False,\n",
    "    'device': 'cuda',\n",
    "    'num_label': len(all_labels),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and params['device'] != 'cpu':\n",
    "    device = torch.device(params['device'])\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "params['device'] = device\n",
    "\n",
    "print('Loading Data...')\n",
    "params['unique_domains'] = list(all_data.corpus.unique()) + ['vaccine']\n",
    "\n",
    "# build tokenizer and weight\n",
    "tok_dir = os.path.dirname(params['model_dir'])\n",
    "params['tok_dir'] = tok_dir\n",
    "params['word_emb_path'] = os.path.join(\n",
    "    tok_dir, params['dname'] + '.npy'\n",
    ")\n",
    "tok = build_tok(\n",
    "    all_data.text.tolist() + vaccine_df.text.tolist(), max_feature=params['max_feature'],\n",
    "    opath=os.path.join(tok_dir, '{}.tok'.format(params['dname']))\n",
    ")\n",
    "if not os.path.exists(params['word_emb_path']):\n",
    "    build_wt(tok, params['emb_path'], params['word_emb_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 2/50 [00:00<00:03, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/vaccine.txt', 'model_dir': '../resource/model/vaccine/', 'dname': 'vaccine', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 50, 'batch_size': 64, 'lr': 9e-05, 'max_len': 100, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy', 'vaccine'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 10, 'tok_dir': '../resource/model/vaccine', 'word_emb_path': '../resource/model/vaccine/vaccine.npy'}\n",
      "Epoch:  0\n",
      "0.6432646281867767\n",
      "Epoch:  1\n",
      "0.67698670605613\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|â–Š         | 4/50 [00:00<00:03, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67698670605613\n",
      "Epoch:  3\n",
      "0.67698670605613\n",
      "Epoch:  4\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8/50 [00:00<00:02, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "0.67698670605613\n",
      "Epoch:  6\n",
      "0.67698670605613\n",
      "Epoch:  7\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆ        | 10/50 [00:00<00:02, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n",
      "0.67698670605613\n",
      "Epoch:  9\n",
      "0.67698670605613\n",
      "Epoch:  10\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:00<00:02, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "0.67698670605613\n",
      "Epoch:  12\n",
      "0.67698670605613\n",
      "Epoch:  13\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:01<00:02, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14\n",
      "0.67698670605613\n",
      "Epoch:  15\n",
      "0.67698670605613\n",
      "Epoch:  16\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:01<00:02, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17\n",
      "0.67698670605613\n",
      "Epoch:  18\n",
      "0.67698670605613\n",
      "Epoch:  19\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:01<00:02, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20\n",
      "0.67698670605613\n",
      "Epoch:  21\n",
      "0.67698670605613\n",
      "Epoch:  22\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:01<00:01, 14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23\n",
      "0.67698670605613\n",
      "Epoch:  24\n",
      "0.67698670605613\n",
      "Epoch:  25\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:01<00:01, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26\n",
      "0.67698670605613\n",
      "Epoch:  27\n",
      "0.67698670605613\n",
      "Epoch:  28\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:02<00:01, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29\n",
      "0.67698670605613\n",
      "Epoch:  30\n",
      "0.67698670605613\n",
      "Epoch:  31\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:02<00:01, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  32\n",
      "0.67698670605613\n",
      "Epoch:  33\n",
      "0.67698670605613\n",
      "Epoch:  34\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:02<00:00, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  35\n",
      "0.67698670605613\n",
      "Epoch:  36\n",
      "0.67698670605613\n",
      "Epoch:  37\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:02<00:00, 14.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  38\n",
      "0.67698670605613\n",
      "Epoch:  39\n",
      "0.67698670605613\n",
      "Epoch:  40\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:03<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  41\n",
      "0.67698670605613\n",
      "Epoch:  42\n",
      "0.67698670605613\n",
      "Epoch:  43\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:03<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  44\n",
      "0.67698670605613\n",
      "Epoch:  45\n",
      "0.67698670605613\n",
      "Epoch:  46\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:03<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  47\n",
      "0.67698670605613\n",
      "Epoch:  48\n",
      "0.67698670605613\n",
      "Epoch:  49\n",
      "0.67698670605613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vaccine data only\n",
    "params['epochs'] = 50\n",
    "data_encoder = DataEncoder(params, mtype='rnn')\n",
    "\n",
    "# # train_indices, val_indices, test_indices = data_split(data)\n",
    "# train_data = {\n",
    "#     'docs': [data['docs'][item] for item in train_indices],\n",
    "#     'labels': [data['labels'][item] for item in train_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in train_indices],\n",
    "# }\n",
    "# valid_data = {\n",
    "#     'docs': [data['docs'][item] for item in val_indices],\n",
    "#     'labels': [data['labels'][item] for item in val_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in val_indices],\n",
    "# }\n",
    "# test_data = {\n",
    "#     'docs': [data['docs'][item] for item in test_indices],\n",
    "#     'labels': [data['labels'][item] for item in test_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in test_indices],\n",
    "# }\n",
    "# if params['over_sample']:\n",
    "#     ros = RandomOverSampler(random_state=33)\n",
    "#     sample_indices = [[item] for item in range(len(train_data['docs']))]\n",
    "#     sample_indices, _ = ros.fit_resample(sample_indices, train_data['labels'])\n",
    "#     sample_indices = [item[0] for item in sample_indices]\n",
    "#     train_data = {\n",
    "#         'docs': [train_data['docs'][item] for item in sample_indices],\n",
    "#         'labels': [train_data['labels'][item] for item in sample_indices],\n",
    "#         params['domain_name']: [train_data[params['domain_name']][item] for item in sample_indices],\n",
    "#     }\n",
    "\n",
    "train_data = TorchDataset(vaccine_train, params['domain_name'])\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "# valid_data = TorchDataset(vaccine_train, params['domain_name'])\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_data, batch_size=params['batch_size'], shuffle=False,\n",
    "#     collate_fn=data_encoder\n",
    "# )\n",
    "test_data = TorchDataset(vaccine_test, params['domain_name'])\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "\n",
    "# build model\n",
    "rnn_model = RegularRNN(params)\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.RMSprop(rnn_model.parameters(), lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "best_score = 0.\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "    train_loss = 0\n",
    "    rnn_model.train()\n",
    "\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = rnn_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss = criterion(predictions, input_labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss_avg = train_loss / (step + 1)\n",
    "        if (step + 1) % 301 == 0:\n",
    "            print('Epoch: {}, Step: {}'.format(epoch, step))\n",
    "            print('\\tLoss: {}.'.format(loss_avg))\n",
    "            print('-------------------------------------------------')\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate on the valid set\n",
    "#     y_preds = []\n",
    "#     y_trues = []\n",
    "#     rnn_model.eval()\n",
    "#     for valid_batch in valid_data_loader:\n",
    "#         valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "#         input_docs, input_labels, input_domains = valid_batch\n",
    "#         with torch.no_grad():\n",
    "#             predictions = rnn_model(**{\n",
    "#                 'input_docs': input_docs,\n",
    "#             })\n",
    "#         logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "#         y_preds.extend(logits)\n",
    "#         y_trues.extend(input_labels.to('cpu').numpy())\n",
    "    \n",
    "#     eval_score = multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary')\n",
    "#     if eval_score > best_score:\n",
    "#         best_score = eval_score\n",
    "#         torch.save(rnn_model, params['model_dir'] + '{}.pth'.format(os.path.basename(__file__)))\n",
    "\n",
    "    y_preds = []\n",
    "    y_probs = []\n",
    "    y_trues = []\n",
    "    y_domains = []\n",
    "    # evaluate on the test set\n",
    "    for test_batch in test_data_loader:\n",
    "        test_batch = tuple(t.to(device) for t in test_batch)\n",
    "        input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = rnn_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "        logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "        y_preds.extend(logits)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "        y_probs.extend([item[1] for item in logits])\n",
    "        y_domains.extend(input_domains.detach().cpu().numpy())\n",
    "    print('Epoch: ', epoch)\n",
    "#     print(multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary'))\n",
    "    print(micro_f1_average(y_preds=y_preds, y_truths=y_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/vaccine.txt', 'model_dir': '../resource/model/vaccine/', 'dname': 'vaccine', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 15, 'batch_size': 64, 'lr': 9e-05, 'max_len': 100, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy', 'vaccine'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 10, 'tok_dir': '../resource/model/vaccine', 'word_emb_path': '../resource/model/vaccine/vaccine.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|â–‹         | 1/15 [00:05<01:11,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0.6703248729733335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|â–ˆâ–Ž        | 2/15 [00:10<01:07,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0.6543571206991894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆ        | 3/15 [00:15<01:02,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "0.6671585666154024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|â–ˆâ–ˆâ–‹       | 4/15 [00:20<00:57,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "0.6657906263688129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:26<00:52,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "0.6644082613921976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:31<00:47,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "0.6741350286685978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:36<00:42,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n",
      "0.6809760562501299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:42<00:37,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n",
      "0.6710782524035537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:47<00:31,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n",
      "0.6639940255015784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:52<00:25,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n",
      "0.6743091835058321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:57<00:20,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n",
      "0.6855179111725374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [01:02<00:15,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "0.6899440426715533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [01:08<00:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12\n",
      "0.7017387967768645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [01:13<00:05,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13\n",
      "0.7068620572138804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:18<00:00,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14\n",
      "0.708523895271939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_encoder = DataEncoder(params, mtype='rnn')\n",
    "\n",
    "# # train_indices, val_indices, test_indices = data_split(data)\n",
    "# train_data = {\n",
    "#     'docs': [data['docs'][item] for item in train_indices],\n",
    "#     'labels': [data['labels'][item] for item in train_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in train_indices],\n",
    "# }\n",
    "# valid_data = {\n",
    "#     'docs': [data['docs'][item] for item in val_indices],\n",
    "#     'labels': [data['labels'][item] for item in val_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in val_indices],\n",
    "# }\n",
    "# test_data = {\n",
    "#     'docs': [data['docs'][item] for item in test_indices],\n",
    "#     'labels': [data['labels'][item] for item in test_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in test_indices],\n",
    "# }\n",
    "# if params['over_sample']:\n",
    "#     ros = RandomOverSampler(random_state=33)\n",
    "#     sample_indices = [[item] for item in range(len(train_data['docs']))]\n",
    "#     sample_indices, _ = ros.fit_resample(sample_indices, train_data['labels'])\n",
    "#     sample_indices = [item[0] for item in sample_indices]\n",
    "#     train_data = {\n",
    "#         'docs': [train_data['docs'][item] for item in sample_indices],\n",
    "#         'labels': [train_data['labels'][item] for item in sample_indices],\n",
    "#         params['domain_name']: [train_data[params['domain_name']][item] for item in sample_indices],\n",
    "#     }\n",
    "\n",
    "train_data = TorchDataset(all_train, params['domain_name'])\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "valid_data = TorchDataset(vaccine_train, params['domain_name'])\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "test_data = TorchDataset(vaccine_test, params['domain_name'])\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "\n",
    "# build model\n",
    "rnn_model = RegularRNN(params)\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.RMSprop(rnn_model.parameters(), lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "best_score = 0.\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "    train_loss = 0\n",
    "    rnn_model.train()\n",
    "\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = rnn_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss = criterion(predictions, input_labels)\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         loss_avg = train_loss / (step + 1)\n",
    "#         if (step + 1) % 301 == 0:\n",
    "#             print('Epoch: {}, Step: {}'.format(epoch, step))\n",
    "#             print('\\tLoss: {}.'.format(loss_avg))\n",
    "#             print('-------------------------------------------------')\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "    for _ in range(10):\n",
    "        for valid_batch in valid_data_loader:\n",
    "            valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "            input_docs, input_labels, input_domains = valid_batch\n",
    "            optimizer.zero_grad()\n",
    "            predictions = rnn_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "            # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "            loss = criterion(predictions, input_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluate on the valid set\n",
    "#     y_preds = []\n",
    "#     y_trues = []\n",
    "#     rnn_model.eval()\n",
    "#     for valid_batch in valid_data_loader:\n",
    "#         valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "#         input_docs, input_labels, input_domains = valid_batch\n",
    "#         with torch.no_grad():\n",
    "#             predictions = rnn_model(**{\n",
    "#                 'input_docs': input_docs,\n",
    "#             })\n",
    "#         logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "#         y_preds.extend(logits)\n",
    "#         y_trues.extend(input_labels.to('cpu').numpy())\n",
    "    \n",
    "#     eval_score = multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary')\n",
    "#     if eval_score > best_score:\n",
    "#         best_score = eval_score\n",
    "#         torch.save(rnn_model, params['model_dir'] + '{}.pth'.format(os.path.basename(__file__)))\n",
    "\n",
    "    y_preds = []\n",
    "    y_probs = []\n",
    "    y_trues = []\n",
    "    y_domains = []\n",
    "    # evaluate on the test set\n",
    "    rnn_model.eval()\n",
    "    for test_batch in test_data_loader:\n",
    "        test_batch = tuple(t.to(device) for t in test_batch)\n",
    "        input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = rnn_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "        logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "        y_preds.extend(logits)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "        y_probs.extend([item[1] for item in logits])\n",
    "        y_domains.extend(input_domains.detach().cpu().numpy())\n",
    "    print('Epoch: ', epoch)\n",
    "#     print(multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary'))\n",
    "    print(micro_f1_average(y_preds=y_preds, y_truths=y_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/vaccine.txt', 'model_dir': '../resource/model/vaccine/', 'dname': 'vaccine', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 50, 'batch_size': 64, 'lr': 9e-05, 'max_len': 100, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy', 'vaccine'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 10, 'tok_dir': '../resource/model/vaccine', 'word_emb_path': '../resource/model/vaccine/vaccine.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|â–         | 1/50 [00:05<04:21,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0.6703248729733335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|â–         | 2/50 [00:10<04:17,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0.6569765018134319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|â–Œ         | 3/50 [00:16<04:13,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "0.6657323307189895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|â–Š         | 4/50 [00:21<04:09,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "0.6727744800636425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|â–ˆ         | 5/50 [00:27<04:03,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "0.6727744800636425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|â–ˆâ–        | 6/50 [00:32<03:57,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "0.6783432644603227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|â–ˆâ–        | 7/50 [00:37<03:51,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n",
      "0.6825931464147722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|â–ˆâ–Œ        | 8/50 [00:43<03:47,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n",
      "0.6784002037736989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|â–ˆâ–Š        | 9/50 [00:48<03:41,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n",
      "0.6784557412450767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆ        | 10/50 [00:53<03:34,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n",
      "0.6887619687552494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|â–ˆâ–ˆâ–       | 11/50 [00:59<03:30,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n",
      "0.7098205804962632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|â–ˆâ–ˆâ–       | 12/50 [01:04<03:25,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "0.7254899653680671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:10<03:18,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12\n",
      "0.7348574456972564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|â–ˆâ–ˆâ–Š       | 14/50 [01:15<03:12,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13\n",
      "0.739360913827107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:20<03:04,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14\n",
      "0.7404514934361075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:25<02:59,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15\n",
      "0.7065078120917594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:31<02:54,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16\n",
      "0.7420491527325775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:36<02:48,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17\n",
      "0.7267994145210566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:41<02:43,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18\n",
      "0.7357203426889599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:46<02:38,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19\n",
      "0.7409142835739836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:52<02:32,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20\n",
      "0.739819167079484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:57<02:26,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21\n",
      "0.739819167079484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [02:02<02:21,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22\n",
      "0.739242543246298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [02:07<02:16,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23\n",
      "0.7401076883350388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [02:13<02:11,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24\n",
      "0.7402443165756979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [02:18<02:06,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25\n",
      "0.7392681337429443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [02:23<02:00,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26\n",
      "0.7391960997323865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [02:28<01:55,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  27\n",
      "0.7407002831399901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [02:34<01:50,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  28\n",
      "0.7356105181340223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [02:39<01:43,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29\n",
      "0.7350974219995965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [02:44<01:39,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  30\n",
      "0.7373846225928875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [02:49<01:34,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  31\n",
      "0.7387936146848032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [02:55<01:29,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  32\n",
      "0.7321240380605785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [03:00<01:25,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  33\n",
      "0.7326247758020248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [03:05<01:20,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  34\n",
      "0.7278798477918283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [03:11<01:14,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  35\n",
      "0.727201981991376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [03:16<01:09,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  36\n",
      "0.7207520215528785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [03:22<01:04,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  37\n",
      "0.7229772530953066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [03:27<00:59,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  38\n",
      "0.7202507744044693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [03:32<00:53,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  39\n",
      "0.7129904385097263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [03:38<00:48,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  40\n",
      "0.7163352101386397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [03:43<00:43,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  41\n",
      "0.7162790697674419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [03:49<00:37,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  42\n",
      "0.7160997572900911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [03:54<00:32,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  43\n",
      "0.7130584799453589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [03:59<00:26,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  44\n",
      "0.7146417664197704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [04:05<00:21,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  45\n",
      "0.7162790697674419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [04:10<00:16,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  46\n",
      "0.7131897004729374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [04:15<00:10,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  47\n",
      "0.705572598939784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [04:21<00:05,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  48\n",
      "0.7003265691138455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:26<00:00,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  49\n",
      "0.7123357571118766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params['epochs'] = 50\n",
    "\n",
    "data_encoder = DataEncoder(params, mtype='rnn')\n",
    "\n",
    "# # train_indices, val_indices, test_indices = data_split(data)\n",
    "# train_data = {\n",
    "#     'docs': [data['docs'][item] for item in train_indices],\n",
    "#     'labels': [data['labels'][item] for item in train_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in train_indices],\n",
    "# }\n",
    "# valid_data = {\n",
    "#     'docs': [data['docs'][item] for item in val_indices],\n",
    "#     'labels': [data['labels'][item] for item in val_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in val_indices],\n",
    "# }\n",
    "# test_data = {\n",
    "#     'docs': [data['docs'][item] for item in test_indices],\n",
    "#     'labels': [data['labels'][item] for item in test_indices],\n",
    "#     params['domain_name']: [data[params['domain_name']][item] for item in test_indices],\n",
    "# }\n",
    "# if params['over_sample']:\n",
    "#     ros = RandomOverSampler(random_state=33)\n",
    "#     sample_indices = [[item] for item in range(len(train_data['docs']))]\n",
    "#     sample_indices, _ = ros.fit_resample(sample_indices, train_data['labels'])\n",
    "#     sample_indices = [item[0] for item in sample_indices]\n",
    "#     train_data = {\n",
    "#         'docs': [train_data['docs'][item] for item in sample_indices],\n",
    "#         'labels': [train_data['labels'][item] for item in sample_indices],\n",
    "#         params['domain_name']: [train_data[params['domain_name']][item] for item in sample_indices],\n",
    "#     }\n",
    "\n",
    "train_data = TorchDataset(all_train, params['domain_name'])\n",
    "train_data_loader = DataLoader(\n",
    "    train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "valid_data = TorchDataset(vaccine_train, params['domain_name'])\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "test_data = TorchDataset(vaccine_test, params['domain_name'])\n",
    "test_data_loader = DataLoader(\n",
    "    test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=data_encoder\n",
    ")\n",
    "\n",
    "# build model\n",
    "rnn_model = RegularRNN(params)\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.RMSprop(rnn_model.parameters(), lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "best_score = 0.\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "    train_loss = 0\n",
    "    rnn_model.train()\n",
    "\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        optimizer.zero_grad()\n",
    "        predictions = rnn_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss = criterion(predictions, input_labels)\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         loss_avg = train_loss / (step + 1)\n",
    "#         if (step + 1) % 301 == 0:\n",
    "#             print('Epoch: {}, Step: {}'.format(epoch, step))\n",
    "#             print('\\tLoss: {}.'.format(loss_avg))\n",
    "#             print('-------------------------------------------------')\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "    for _ in range(10):\n",
    "        for valid_batch in valid_data_loader:\n",
    "            valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "            input_docs, input_labels, input_domains = valid_batch\n",
    "            optimizer.zero_grad()\n",
    "            predictions = rnn_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "            # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "            loss = criterion(predictions, input_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluate on the valid set\n",
    "#     y_preds = []\n",
    "#     y_trues = []\n",
    "#     rnn_model.eval()\n",
    "#     for valid_batch in valid_data_loader:\n",
    "#         valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "#         input_docs, input_labels, input_domains = valid_batch\n",
    "#         with torch.no_grad():\n",
    "#             predictions = rnn_model(**{\n",
    "#                 'input_docs': input_docs,\n",
    "#             })\n",
    "#         logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "#         y_preds.extend(logits)\n",
    "#         y_trues.extend(input_labels.to('cpu').numpy())\n",
    "    \n",
    "#     eval_score = multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary')\n",
    "#     if eval_score > best_score:\n",
    "#         best_score = eval_score\n",
    "#         torch.save(rnn_model, params['model_dir'] + '{}.pth'.format(os.path.basename(__file__)))\n",
    "\n",
    "    y_preds = []\n",
    "    y_probs = []\n",
    "    y_trues = []\n",
    "    y_domains = []\n",
    "    # evaluate on the test set\n",
    "    rnn_model.eval()\n",
    "    for test_batch in test_data_loader:\n",
    "        test_batch = tuple(t.to(device) for t in test_batch)\n",
    "        input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = rnn_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "        logits = (torch.sigmoid(predictions) > .5).long().cpu().numpy()\n",
    "        y_preds.extend(logits)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "        y_probs.extend([item[1] for item in logits])\n",
    "        y_domains.extend(input_domains.detach().cpu().numpy())\n",
    "    print('Epoch: ', epoch)\n",
    "    test_score = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "#     print(multi_label_f1(y_preds=y_preds, y_truths=y_trues, mode='binary'))\n",
    "    print(test_score)\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        torch.save(rnn_model, params['model_dir'] + '{}.pth'.format('vaccine_rnn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7420491527325775"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#         with open(params['result_path'], 'a') as wfile:\n",
    "#             wfile.write('{}...............................\\n'.format(datetime.datetime.now()))\n",
    "#             wfile.write('Performance Evaluation for the task: {}\\n'.format(params['dname']))\n",
    "#             wfile.write('F1-weighted score: {}\\n'.format(\n",
    "#                 metrics.f1_score(y_true=y_trues, y_pred=y_preds, average='weighted')\n",
    "#             ))\n",
    "#             fpr, tpr, _ = metrics.roc_curve(y_true=y_trues, y_score=y_probs)\n",
    "#             wfile.write('AUC score: {}\\n'.format(\n",
    "#                 metrics.auc(fpr, tpr)\n",
    "#             ))\n",
    "#             report = metrics.classification_report(\n",
    "#                 y_true=y_trues, y_pred=y_preds, digits=3\n",
    "#             )\n",
    "#             print(report)\n",
    "#             wfile.write(report)\n",
    "#             wfile.write('\\n')\n",
    "\n",
    "#             wfile.write('Fairness Evaluation\\n')\n",
    "\n",
    "#             wfile.write('...............................\\n\\n')\n",
    "#             wfile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
