{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gensim\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "spw_set = set(stopwords.words('english'))\n",
    "spw_set.add('url')\n",
    "tokenizer = TweetTokenizer()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess a single tweet\n",
    "    :param tweet:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    tweet = re.sub(r\"https?:\\S+\", \"URL\", tweet)  # replace url\n",
    "    # replace user\n",
    "    # tweet = re.sub(r'@\\w+', 'USER', tweet)\n",
    "    # replace hashtag\n",
    "    # tweet = re.sub(r'#\\S+', 'HASHTAG', tweet)\n",
    "    # tokenize\n",
    "    return [item.strip() for item in tokenizer.tokenize(tweet) if len(item.strip()) > 0]\n",
    "\n",
    "\n",
    "def label_encoder(raw_label):\n",
    "    pre_labels = [\n",
    "        'subversion', 'loyalty', 'care', 'cheating',\n",
    "        'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "    ]\n",
    "    encode_label = [0]*(len(pre_labels) + 1)\n",
    "    if type(raw_label) != str:\n",
    "        encode_label[-1] = 1\n",
    "        return encode_label\n",
    "    for label in raw_label.split(','):\n",
    "        if label not in pre_labels:\n",
    "            encode_label[-1] = 1\n",
    "        else:\n",
    "            encode_label[pre_labels.index(label)] = 1\n",
    "    return encode_label\n",
    "\n",
    "\n",
    "def micro_f1_average(y_preds, y_truths):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for idx, (y_pred, y_truth) in enumerate(zip(y_preds, y_truths)):\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        true_positives = np.sum(np.logical_and(y_truth, y_pred))\n",
    "\n",
    "        # compute the sum of tp + fp across training examples and labels\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        l_prec_den = np.sum(y_pred)\n",
    "        if l_prec_den != 0:\n",
    "            # compute micro-averaged precision\n",
    "            precisions.append(true_positives / l_prec_den)\n",
    "\n",
    "        # compute sum of tp + fn across training examples and labels\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        l_recall_den = np.sum(y_truth)\n",
    "\n",
    "        # compute mirco-average recall\n",
    "        if l_recall_den != 0:\n",
    "            recalls.append(true_positives / l_recall_den)\n",
    "\n",
    "    precisions = np.average(precisions)\n",
    "    recalls = np.average(recalls)\n",
    "    if precisions + recalls == 0:\n",
    "        return 0\n",
    "    f1 = 2 * precisions * recalls / (precisions + recalls)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def multi_label_f1(y_preds, y_truths, mode='weighted'):\n",
    "    preds = dict()\n",
    "    truths = dict()\n",
    "    for idx in range(len(y_truths)):\n",
    "        for jdx in range(len(y_truths[idx])):\n",
    "            if jdx not in preds:\n",
    "                preds[jdx] = []\n",
    "                truths[jdx] = []\n",
    "            preds[jdx].append(y_preds[idx][jdx])\n",
    "            truths[jdx].append(y_truths[idx][jdx])\n",
    "    results = []\n",
    "    for jdx in preds:\n",
    "        results.append(metrics.f1_score(preds[jdx], truths[jdx], average=mode))\n",
    "    return np.average(results)\n",
    "\n",
    "\n",
    "def build_wt(tkn, emb_path, opath):\n",
    "    \"\"\"Build weight using word embedding\"\"\"\n",
    "    embed_len = len(tkn.word_index)\n",
    "    if embed_len > tkn.num_words:\n",
    "        embed_len = tkn.num_words\n",
    "\n",
    "    if emb_path.endswith('.bin'):\n",
    "        embeds = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            emb_path, binary=True, unicode_errors='ignore'\n",
    "        )\n",
    "        emb_size = embeds.vector_size\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        for pair in zip(embeds.wv.index2word, embeds.wv.syn0):\n",
    "            if pair[0] in tkn.word_index and \\\n",
    "                    tkn.word_index[pair[0]] < tkn.num_words:\n",
    "                emb_matrix[tkn.word_index[pair[0]]] = np.asarray([\n",
    "                    float(item) for item in pair[1]\n",
    "                ], dtype=np.float32)\n",
    "    else:\n",
    "        dfile = open(emb_path)\n",
    "        line = dfile.readline().strip().split()\n",
    "        if len(line) < 5:\n",
    "            line = dfile.readline().strip().split()\n",
    "        emb_size = len(line[1:])\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        dfile.close()\n",
    "\n",
    "        with open(emb_path) as dfile:\n",
    "            for line in dfile:\n",
    "                line = line.strip().split()\n",
    "                if line[0] in tkn.word_index and \\\n",
    "                        tkn.word_index[line[0]] < tkn.num_words:\n",
    "                    emb_matrix[tkn.word_index[line[0]]] = np.asarray([\n",
    "                        float(item) for item in line[1:]\n",
    "                    ], dtype=np.float32)\n",
    "    # emb_matrix = np.array(emb_matrix, dtype=np.float32)\n",
    "    np.save(opath, emb_matrix)\n",
    "    return emb_matrix\n",
    "\n",
    "\n",
    "def build_tok(docs, max_feature, opath):\n",
    "    if os.path.exists(opath):\n",
    "        return pickle.load(open(opath, 'rb'))\n",
    "    else:\n",
    "        # load corpus\n",
    "        tkn = Tokenizer(num_words=max_feature)\n",
    "        tkn.fit_on_texts(docs)\n",
    "\n",
    "        with open(opath, 'wb') as wfile:\n",
    "            pickle.dump(tkn, wfile)\n",
    "        return tkn\n",
    "\n",
    "\n",
    "class DataEncoder(object):\n",
    "    def __init__(self, params, mtype='rnn'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param params:\n",
    "        :param mtype: Model type, rnn or bert\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.mtype = mtype\n",
    "        if self.mtype == 'rnn':\n",
    "            self.tok = pickle.load(open(\n",
    "                os.path.join(params['tok_dir'], '{}.tok'.format(params['dname'])), 'rb'))\n",
    "        elif self.mtype == 'bert':\n",
    "            self.tok = BertTokenizer.from_pretrained(params['bert_name'])\n",
    "        else:\n",
    "            raise ValueError('Only support BERT and RNN data encoders')\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        docs = []\n",
    "        labels = []\n",
    "        domains = []\n",
    "        for text, label, domain in batch:\n",
    "            if self.mtype == 'bert':\n",
    "                text = self.tok.encode_plus(\n",
    "                    text, padding='max_length', max_length=self.params['max_len'],\n",
    "                    return_tensors='pt', return_token_type_ids=False,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                docs.append(text['input_ids'][0])\n",
    "            else:\n",
    "                docs.append(text)\n",
    "            labels.append(label)\n",
    "            domains.append(domain)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        domains = torch.tensor(domains, dtype=torch.long)\n",
    "        if self.mtype == 'rnn':\n",
    "            # padding and tokenize\n",
    "            docs = self.tok.texts_to_sequences(docs)\n",
    "            docs = pad_sequences(docs)\n",
    "            docs = torch.Tensor(docs).long()\n",
    "        else:\n",
    "            docs = torch.stack(docs).long()\n",
    "        return docs, labels, domains\n",
    "\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, dataset, domain_name):\n",
    "        self.dataset = dataset\n",
    "        self.domain_name = domain_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['docs'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.domain_name in self.dataset:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], self.dataset[self.domain_name][idx]\n",
    "        else:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], -1\n",
    "\n",
    "\n",
    "class RegularRNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RegularRNN, self).__init__()\n",
    "        self.params = params\n",
    "\n",
    "        if 'word_emb_path' in self.params and os.path.exists(self.params['word_emb_path']):\n",
    "            self.wemb = nn.Embedding.from_pretrained(\n",
    "                torch.FloatTensor(np.load(\n",
    "                    self.params['word_emb_path'], allow_pickle=True))\n",
    "            )\n",
    "        else:\n",
    "            self.wemb = nn.Embedding(\n",
    "                self.params['max_feature'], self.params['emb_dim']\n",
    "            )\n",
    "            self.wemb.reset_parameters()\n",
    "            nn.init.kaiming_uniform_(self.wemb.weight, a=np.sqrt(5))\n",
    "\n",
    "        if self.params['bidirectional']:\n",
    "            self.word_hidden_size = self.params['emb_dim'] // 2\n",
    "        else:\n",
    "            self.word_hidden_size = self.params['emb_dim']\n",
    "\n",
    "        # domain adaptation\n",
    "        self.doc_net_general = nn.GRU(\n",
    "            self.wemb.embedding_dim, self.word_hidden_size,\n",
    "            bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        # prediction\n",
    "        self.predictor = nn.Linear(\n",
    "            self.params['emb_dim'], self.params['num_label'])\n",
    "\n",
    "    def forward(self, input_docs):\n",
    "        # encode the document from different perspectives\n",
    "        doc_embs = self.wemb(input_docs)\n",
    "        _, doc_general = self.doc_net_general(doc_embs)  # omit hidden vectors\n",
    "\n",
    "        # concatenate hidden state\n",
    "        if self.params['bidirectional']:\n",
    "            doc_general = torch.cat((doc_general[0, :, :], doc_general[1, :, :]), -1)\n",
    "\n",
    "        if doc_general.shape[0] == 1:\n",
    "            doc_general = doc_general.squeeze(dim=0)\n",
    "\n",
    "        # prediction\n",
    "        doc_preds = self.predictor(doc_general)\n",
    "        return doc_preds\n",
    "\n",
    "\n",
    "class AdaptRNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(AdaptRNN, self).__init__()\n",
    "        self.params = params\n",
    "\n",
    "        if 'word_emb_path' in self.params and os.path.exists(self.params['word_emb_path']):\n",
    "            self.wemb = nn.Embedding.from_pretrained(\n",
    "                torch.FloatTensor(np.load(\n",
    "                    self.params['word_emb_path'], allow_pickle=True))\n",
    "            )\n",
    "        else:\n",
    "            self.wemb = nn.Embedding(\n",
    "                self.params['max_feature'], self.params['emb_dim']\n",
    "            )\n",
    "            self.wemb.reset_parameters()\n",
    "            nn.init.kaiming_uniform_(self.wemb.weight, a=np.sqrt(5))\n",
    "\n",
    "        if self.params['bidirectional']:\n",
    "            self.word_hidden_size = self.params['emb_dim'] // 2\n",
    "        else:\n",
    "            self.word_hidden_size = self.params['emb_dim']\n",
    "\n",
    "        # domain adaptation\n",
    "        self.domain_net = nn.GRU(\n",
    "            self.wemb.embedding_dim, self.word_hidden_size,\n",
    "            bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        # two domains, this domain vs others\n",
    "        self.domain_clf = nn.Linear(\n",
    "            self.params['emb_dim'], 2\n",
    "        )\n",
    "\n",
    "        # regular prediction\n",
    "        self.document_net = nn.GRU(\n",
    "            self.wemb.embedding_dim, self.word_hidden_size,\n",
    "            bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        # prediction\n",
    "        self.document_predictor = nn.Linear(\n",
    "            self.params['emb_dim'], self.params['num_label'])\n",
    "\n",
    "    def forward(self, input_docs):\n",
    "        # encode the document from different perspectives\n",
    "        doc_embs = self.wemb(input_docs)\n",
    "        _, doc_general = self.document_net(doc_embs)  # omit hidden vectors\n",
    "\n",
    "        # concatenate hidden state\n",
    "        if self.params['bidirectional']:\n",
    "            doc_general = torch.cat((doc_general[0, :, :], doc_general[1, :, :]), -1)\n",
    "\n",
    "        if doc_general.shape[0] == 1:\n",
    "            doc_general = doc_general.squeeze(dim=0)\n",
    "\n",
    "        # prediction\n",
    "        doc_preds = self.document_predictor(doc_general)\n",
    "        return doc_preds\n",
    "\n",
    "    def discriminator(self, input_docs):\n",
    "        # encode the document from different perspectives\n",
    "        doc_embs = self.wemb(input_docs)\n",
    "        _, doc_domain = self.domain_net(doc_embs)  # omit hidden vectors\n",
    "        # concatenate hidden state\n",
    "        if self.params['bidirectional']:\n",
    "            doc_domain = torch.cat((doc_domain[0, :, :], doc_domain[1, :, :]), -1)\n",
    "\n",
    "        if doc_domain.shape[0] == 1:\n",
    "            doc_domain = doc_domain.squeeze(dim=0)\n",
    "\n",
    "        # prediction\n",
    "        domain_preds = self.domain_clf(doc_domain)\n",
    "        return domain_preds\n",
    "\n",
    "    def freeze_layer(self, if_train=True):\n",
    "        self.wemb.weight.requires_grad = if_train\n",
    "\n",
    "\n",
    "def data_split(data):\n",
    "    \"\"\"\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_indices = list(range(len(data['docs'])))\n",
    "    np.random.seed(33)  # for reproductive results\n",
    "    np.random.shuffle(data_indices)\n",
    "\n",
    "    train_indices = data_indices[:int(.8 * len(data_indices))]\n",
    "    dev_indices = data_indices[int(.8 * len(data_indices)):int(.9 * len(data_indices))]\n",
    "    test_indices = data_indices[int(.9 * len(data_indices)):]\n",
    "    return train_indices, dev_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_morality = [\n",
    "    'subversion', 'loyalty', 'care', 'cheating',\n",
    "    'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "]\n",
    "\n",
    "result_dir = '../resource/results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "model_dir = '../resource/model/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "model_dir = model_dir + 'adapt_rnn/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "params = {\n",
    "    'result_path': os.path.join(result_dir, 'adapt_rnn.txt'),\n",
    "    'model_dir': model_dir,\n",
    "    'dname': 'all',\n",
    "    'dpath': '../data/dataset.tsv',\n",
    "    'max_feature': 15000,\n",
    "    'over_sample': True,\n",
    "    'domain_name': 'corpus',\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'lr': 9e-5,\n",
    "    'max_len': 60,\n",
    "    'dp_rate': .2,\n",
    "    'optimizer': 'rmsprop',\n",
    "    'emb_path': '/data/models/glove.twitter.27B.200d.txt',  # adjust for different languages\n",
    "    'emb_dim': 200,\n",
    "    'unique_domains': [],\n",
    "    'bidirectional': False,\n",
    "    'device': 'cuda',\n",
    "    'num_label': len(all_morality)+1,  # plus no-moral\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "all_labels = [\n",
    "    'subversion', 'loyalty', 'care', 'cheating',\n",
    "    'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "]\n",
    "wfile = open(params['result_path'], 'a')\n",
    "\n",
    "print('Loading Data...')\n",
    "all_data = pd.read_csv(params['dpath'], sep='\\t', dtype=str)\n",
    "all_data.tid = all_data.tid.apply(lambda x: str(x))\n",
    "all_data = all_data[~all_data.text.isna()]\n",
    "all_data = all_data[~all_data.labels.isna()]\n",
    "# preprocess tweet and remove short tweet\n",
    "all_data.text = all_data.text.apply(lambda x: preprocess(x))\n",
    "all_data = all_data[all_data.text.apply(lambda x: len(x) > 3)]\n",
    "all_data.text = all_data.text.apply(lambda x: ' '.join(x))\n",
    "all_data.labels = all_data.labels.apply(lambda x: label_encoder(x))\n",
    "params['unique_domains'] = list(all_data.corpus.unique())\n",
    "wfile.write(json.dumps(params) + '\\n')\n",
    "\n",
    "if torch.cuda.is_available() and params['device'] != 'cpu':\n",
    "    device = torch.device(params['device'])\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "params['device'] = device\n",
    "\n",
    "# load the vaccine data and test the classifier on the vaccine data\n",
    "vaccine_df = pd.read_csv('../data/vaccine_morality.csv', dtype=str)\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: preprocess(x))\n",
    "# vaccine_df = vaccine_df[vaccine_df.text.apply(lambda x: len(x) > 3)]\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: ' '.join(x))\n",
    "vaccine_df = vaccine_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# domains\n",
    "domain_encoder = list(all_data.corpus.unique()) + ['vaccine']\n",
    "\n",
    "# use half of the vaccine as train and half as test\n",
    "all_corpus = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': all_data.corpus.to_list(),\n",
    "}\n",
    "all_corpus['corpus'] = [domain_encoder.index(item) for item in all_corpus['corpus']]\n",
    "\n",
    "# build tokenizer and weight\n",
    "tok_dir = os.path.dirname(params['model_dir'])\n",
    "params['tok_dir'] = tok_dir\n",
    "params['word_emb_path'] = os.path.join(\n",
    "    tok_dir, params['dname'] + '.npy'\n",
    ")\n",
    "tok = build_tok(\n",
    "    all_data.text.tolist() + vaccine_df.text.tolist(), max_feature=params['max_feature'],\n",
    "    opath=os.path.join(tok_dir, '{}.tok'.format(params['dname']))\n",
    ")\n",
    "if not os.path.exists(params['word_emb_path']):\n",
    "    build_wt(tok, params['emb_path'], params['word_emb_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run over domains...\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:07<02:23,  7.54s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:15<02:17,  7.67s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:23<02:11,  7.76s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:31<02:06,  7.93s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:39<02:00,  8.00s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:48<01:53,  8.07s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:56<01:45,  8.13s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:04<01:38,  8.19s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:12<01:29,  8.18s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:21<01:21,  8.19s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:28<01:12,  8.04s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:36<01:04,  8.04s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:44<00:56,  8.03s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:53<00:48,  8.07s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [02:01<00:40,  8.11s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:09<00:32,  8.15s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:17<00:24,  8.15s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:25<00:16,  8.18s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:34<00:08,  8.15s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:42<00:00,  8.12s/it]\u001b[A\n",
      " 14%|█▍        | 1/7 [02:42<16:15, 162.56s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain ALM, F1-micro-average 0.7155275218077524, Valid Score 0.699893115870314\n",
      "\n",
      "Best on InDomain RNN, Domain ALM, F1-micro-average 0.001464128843338214, Valid Score 0.005865102639296187\n",
      "\n",
      "Best on Adapt RNN, Domain ALM, F1-micro-average 0.7454616041957739, Valid Score 0.742417632674084\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:41,  8.51s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:16<02:33,  8.50s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:25<02:24,  8.52s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:33<02:15,  8.49s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:42<02:06,  8.41s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:50<01:56,  8.34s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:58<01:47,  8.27s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:06<01:39,  8.26s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:14<01:29,  8.18s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:23<01:22,  8.22s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:31<01:13,  8.21s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:38<01:04,  8.07s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:46<00:55,  7.94s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:54<00:47,  7.91s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [02:02<00:40,  8.04s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:10<00:31,  7.95s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:18<00:24,  8.02s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:26<00:15,  7.99s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:34<00:08,  8.05s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:42<00:00,  8.15s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [05:25<13:33, 162.71s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Baltimore, F1-micro-average 0.6743706648348701, Valid Score 0.6491367856257377\n",
      "\n",
      "Best on InDomain RNN, Domain Baltimore, F1-micro-average 0.6332778978767011, Valid Score 0.6353458247129092\n",
      "\n",
      "Best on Adapt RNN, Domain Baltimore, F1-micro-average 0.7055036261494569, Valid Score 0.6968092883212811\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:35,  8.19s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:16<02:26,  8.16s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:24<02:19,  8.19s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:32<02:11,  8.20s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:41<02:03,  8.22s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:49<01:55,  8.22s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:57<01:45,  8.13s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:05<01:38,  8.19s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:13<01:30,  8.25s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:22<01:23,  8.32s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:30<01:14,  8.30s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:38<01:05,  8.21s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:46<00:57,  8.16s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:54<00:47,  7.97s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [02:02<00:40,  8.00s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:10<00:32,  8.02s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:18<00:23,  7.94s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:26<00:16,  8.05s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:33<00:07,  7.70s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:41<00:00,  8.07s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [08:07<10:49, 162.36s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain BLM, F1-micro-average 0.7111520759841282, Valid Score 0.7050893193323013\n",
      "\n",
      "Best on InDomain RNN, Domain BLM, F1-micro-average 0.008377759004088133, Valid Score 0.011764705882352941\n",
      "\n",
      "Best on Adapt RNN, Domain BLM, F1-micro-average 0.8159944363109108, Valid Score 0.8213396312828115\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:42,  8.54s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:16<02:31,  8.43s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:24<02:22,  8.36s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:32<02:10,  8.13s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:40<02:01,  8.09s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:48<01:52,  8.06s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:56<01:44,  8.03s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:04<01:36,  8.00s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:12<01:26,  7.90s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:20<01:19,  7.95s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:27<01:11,  7.92s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:35<01:03,  7.89s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:43<00:54,  7.80s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:51<00:47,  7.97s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:58<00:38,  7.61s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:05<00:29,  7.29s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:12<00:22,  7.44s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:20<00:15,  7.60s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:28<00:07,  7.73s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:36<00:00,  7.83s/it]\u001b[A\n",
      " 57%|█████▋    | 4/7 [10:43<08:02, 160.67s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Davidson, F1-micro-average 0.9316992178584683, Valid Score 0.917618425578129\n",
      "\n",
      "Best on InDomain RNN, Domain Davidson, F1-micro-average 0.955416962543151, Valid Score 0.9495735301789349\n",
      "\n",
      "Best on Adapt RNN, Domain Davidson, F1-micro-average 0.9554169625431509, Valid Score 0.9495735301789348\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:41,  8.47s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:16<02:27,  8.20s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:23<02:13,  7.87s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:31<02:06,  7.92s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:39<01:58,  7.92s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:47<01:51,  7.99s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:55<01:43,  7.94s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:03<01:35,  7.99s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:11<01:28,  8.00s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:19<01:20,  8.04s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:26<01:10,  7.78s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:34<01:02,  7.77s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:41<00:54,  7.76s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:50<00:47,  7.88s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:58<00:39,  7.94s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:06<00:31,  8.00s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:13<00:23,  7.82s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:22<00:16,  8.00s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:29<00:07,  7.78s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:37<00:00,  7.87s/it]\u001b[A\n",
      " 71%|███████▏  | 5/7 [13:21<05:19, 159.74s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Election, F1-micro-average 0.6533191104143782, Valid Score 0.6644830425993861\n",
      "\n",
      "Best on InDomain RNN, Domain Election, F1-micro-average 0.5507247335643304, Valid Score 0.5474455764861101\n",
      "\n",
      "Best on Adapt RNN, Domain Election, F1-micro-average 0.7122923210240693, Valid Score 0.725152493720847\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:32,  8.02s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:15<02:21,  7.86s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:23<02:12,  7.79s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:30<02:04,  7.81s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:38<01:56,  7.74s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:46<01:48,  7.76s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:53<01:39,  7.63s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:00<01:29,  7.50s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:07<01:19,  7.25s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:14<01:11,  7.11s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:21<01:04,  7.17s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:28<00:56,  7.00s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:35<00:50,  7.17s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:43<00:43,  7.29s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:50<00:36,  7.30s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [01:58<00:30,  7.54s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:05<00:22,  7.38s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:12<00:14,  7.29s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:19<00:07,  7.23s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:26<00:00,  7.33s/it]\u001b[A\n",
      " 86%|████████▌ | 6/7 [15:48<02:35, 155.84s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain MeToo, F1-micro-average 0.5490175869617743, Valid Score 0.5528548863675999\n",
      "\n",
      "Best on InDomain RNN, Domain MeToo, F1-micro-average 0.017442399070938457, Valid Score 0.013340337417691828\n",
      "\n",
      "Best on Adapt RNN, Domain MeToo, F1-micro-average 0.5782274805419036, Valid Score 0.5888282451846885\n",
      "\n",
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 20, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:08<02:38,  8.36s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:16<02:29,  8.32s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:24<02:20,  8.29s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:33<02:13,  8.34s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:41<02:05,  8.34s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:50<01:57,  8.40s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:58<01:48,  8.31s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:06<01:39,  8.32s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:14<01:30,  8.18s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:22<01:21,  8.12s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:29<01:09,  7.76s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:37<01:03,  7.89s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:45<00:55,  7.92s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:53<00:47,  7.88s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [02:01<00:40,  8.03s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [02:09<00:32,  8.03s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:16<00:23,  7.78s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:25<00:15,  7.97s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:33<00:08,  8.07s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:41<00:00,  8.08s/it]\u001b[A\n",
      "100%|██████████| 7/7 [18:29<00:00, 158.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Sandy, F1-micro-average 0.5261670447506709, Valid Score 0.5184896743675633\n",
      "\n",
      "Best on InDomain RNN, Domain Sandy, F1-micro-average 0.026998100671439578, Valid Score 0.02502406159769009\n",
      "\n",
      "Best on Adapt RNN, Domain Sandy, F1-micro-average 0.5497819616539511, Valid Score 0.5343784838536557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Run over domains...')\n",
    "for didx, domain in enumerate(tqdm(params['unique_domains'])):\n",
    "    wfile.write('Working on Domain {}, Domain index {} \\n'.format(domain, didx))\n",
    "    in_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] == didx]\n",
    "    out_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] != didx]\n",
    "\n",
    "    train_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in out_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in out_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in out_domain_indices],\n",
    "    }\n",
    "    domain_corpus = {\n",
    "        'docs': [item for item in train_corpus['docs']],\n",
    "        'labels': [item for item in train_corpus['labels']],\n",
    "        'corpus': [0] * len(train_corpus['docs']),  # first collect documents from out of domain\n",
    "    }\n",
    "    in_domain_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in in_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in in_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in in_domain_indices],\n",
    "    }\n",
    "    domain_corpus['docs'].extend(in_domain_corpus['docs'])\n",
    "    domain_corpus['labels'].extend(in_domain_corpus['labels'])\n",
    "    domain_corpus['corpus'].extend([1] * len(in_domain_corpus['docs']))\n",
    "\n",
    "    # 10% for training, 10% for valid, the rest for testing\n",
    "    test_indices, val_indices, train_indices = data_split(in_domain_corpus)\n",
    "    in_domain_train = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in train_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in train_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in train_indices]\n",
    "    }\n",
    "    train_corpus['docs'].extend(in_domain_train['docs'])\n",
    "    train_corpus['labels'].extend(in_domain_train['labels'])\n",
    "    train_corpus['corpus'].extend(in_domain_train['corpus'])\n",
    "\n",
    "    valid_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in val_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in val_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in val_indices]\n",
    "    }\n",
    "    test_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in test_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in test_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in test_indices]\n",
    "    }\n",
    "\n",
    "    # start to iteratively train and test the proposed approach.\n",
    "    train_data = TorchDataset(train_corpus, params['domain_name'])\n",
    "    valid_data = TorchDataset(valid_corpus, params['domain_name'])\n",
    "    test_data = TorchDataset(test_corpus, params['domain_name'])\n",
    "    in_domain_train_data = TorchDataset(in_domain_train, params['domain_name'])\n",
    "    domain_data = TorchDataset(domain_corpus, params['domain_name'])\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='rnn')\n",
    "    )\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_data, batch_size=params['batch_size'], shuffle=False,\n",
    "        collate_fn=DataEncoder(params, mtype='rnn')\n",
    "    )\n",
    "    test_data_loader = DataLoader(\n",
    "        test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "        collate_fn=DataEncoder(params, mtype='rnn')\n",
    "    )\n",
    "    in_domain_train_data_loader = DataLoader(\n",
    "        in_domain_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='rnn')\n",
    "    )\n",
    "    domain_data_loader = DataLoader(\n",
    "        domain_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='rnn')\n",
    "    )\n",
    "\n",
    "    regular_model = RegularRNN(params)\n",
    "    regular_model = regular_model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    regular_optim = torch.optim.RMSprop(regular_model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    indomain_model = RegularRNN(params)\n",
    "    indomain_model = indomain_model.to(device)\n",
    "    indomain_optim = torch.optim.RMSprop(indomain_model.parameters(), lr=params['lr'])\n",
    "\n",
    "    adapt_model = AdaptRNN(params)\n",
    "    adapt_model = adapt_model.to(device)\n",
    "    domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "    pred_params = [param for name, param in adapt_model.named_parameters() if 'domain' not in name]\n",
    "    adapt_pred_optim = torch.optim.RMSprop(pred_params, lr=params['lr'])\n",
    "    domain_params = [param for name, param in adapt_model.named_parameters() if 'domain' in name]\n",
    "    adapt_domain_optim = torch.optim.RMSprop(domain_params, lr=params['lr'])\n",
    "\n",
    "    # train the networks\n",
    "    print('Start to train...')\n",
    "    print(params)\n",
    "    best_valid_regular = 0.\n",
    "    best_valid_adapt = 0.\n",
    "    best_valid_indomain = 0.\n",
    "    \n",
    "    best_test_regular = 0.\n",
    "    best_test_adapt = 0.\n",
    "    best_test_indomain = 0.\n",
    "\n",
    "    for epoch in tqdm(range(params['epochs'])):\n",
    "        train_loss_regular = 0.\n",
    "        train_loss_adapt = 0.\n",
    "        adapt_model.train()\n",
    "        regular_model.train()\n",
    "        indomain_model.train()\n",
    "        \n",
    "        # train indomain model for comparison\n",
    "        for step, train_batch in enumerate(in_domain_train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            indomain_optim.zero_grad()\n",
    "            \n",
    "            # indomain models\n",
    "            indomain_preds = indomain_model(**{'input_docs': input_docs})\n",
    "            loss = criterion(indomain_preds, input_labels)\n",
    "            loss.backward()\n",
    "            indomain_optim.step()\n",
    "            \n",
    "        # train discriminator first\n",
    "        for step, train_batch in enumerate(domain_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            adapt_domain_optim.zero_grad()\n",
    "            domain_preds = adapt_model.discriminator(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "            domain_loss.backward()\n",
    "            adapt_domain_optim.step()\n",
    "\n",
    "        # train predictor\n",
    "        for step, train_batch in enumerate(train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            regular_optim.zero_grad()\n",
    "            adapt_pred_optim.zero_grad()\n",
    "            # adapt_domain_optim.zero_grad()\n",
    "\n",
    "            # regular models\n",
    "            regular_preds = regular_model(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            loss = criterion(regular_preds, input_labels)\n",
    "            train_loss_regular += loss.item()\n",
    "            loss_avg_regular = train_loss_regular / (step + 1)\n",
    "\n",
    "            # adapt models\n",
    "            adapt_preds = adapt_model(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "            domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "            loss_adapt = loss_adapt.mean(axis=1)\n",
    "            loss_adapt = domain_preds[:, 1] * loss_adapt\n",
    "            loss_adapt = loss_adapt.mean()\n",
    "            train_loss_adapt += loss_adapt.item()\n",
    "            loss_avg_adapt = train_loss_adapt / (step + 1)\n",
    "\n",
    "#             if (step + 1) % 301 == 0:\n",
    "#                 print('Epoch: {}, Step: {}'.format(epoch, step))\n",
    "#                 print('\\tRegular Loss: {}.'.format(loss_avg_regular))\n",
    "#                 print('\\tAdapt Loss: {}.'.format(loss_avg_adapt))\n",
    "#                 print('-------------------------------------------------')\n",
    "\n",
    "            loss_adapt.backward()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "            regular_optim.step()\n",
    "            adapt_pred_optim.step()\n",
    "\n",
    "        # fit on in domain corpus.\n",
    "        for _ in range(3):\n",
    "            for step, train_batch in enumerate(in_domain_train_data_loader):\n",
    "                train_batch = tuple(t.to(device) for t in train_batch)\n",
    "                input_docs, input_labels, input_domains = train_batch\n",
    "                adapt_pred_optim.zero_grad()\n",
    "                adapt_preds = adapt_model(**{\n",
    "                    'input_docs': input_docs\n",
    "                })\n",
    "                loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "                loss_adapt = loss_adapt.mean()\n",
    "                loss_adapt.backward()\n",
    "                adapt_pred_optim.step()\n",
    "        \n",
    "        # evaluate on valid data\n",
    "        regular_model.eval()\n",
    "        adapt_model.eval()\n",
    "        indomain_model.eval()\n",
    "        y_preds_regular = []\n",
    "        y_preds_adapt = []\n",
    "        y_preds_indomain = []\n",
    "        y_trues = []\n",
    "        for valid_batch in valid_data_loader:\n",
    "            valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "            input_docs, input_labels, input_domains = valid_batch\n",
    "            with torch.no_grad():\n",
    "                preds_regular = regular_model(**{'input_docs': input_docs})\n",
    "                preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "                preds_indomain = indomain_model(**{'input_docs': input_docs})\n",
    "\n",
    "            logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "            logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "            logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "            \n",
    "            y_preds_regular.extend(logits_regular)\n",
    "            y_preds_adapt.extend(logits_adapt)\n",
    "            y_preds_indomain.extend(logits_indomain)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        eval_score_regular = micro_f1_average(y_preds=y_preds_regular, y_truths=y_trues)\n",
    "        eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "        eval_score_indomain = micro_f1_average(y_preds=y_preds_indomain, y_truths=y_trues)\n",
    "\n",
    "        # test for regular model\n",
    "        if eval_score_regular > best_valid_regular:\n",
    "            best_valid_regular = eval_score_regular\n",
    "            torch.save(regular_model, params['model_dir'] + 'regular_model_moral.pth')\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_regular = regular_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_regular)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_regular = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_regular < test_score_regular:\n",
    "                best_test_regular = test_score_regular\n",
    "            regular_results = 'Test on Regular RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_regular, best_valid_regular)\n",
    "            # print('Regular Results: ', regular_results)\n",
    "            wfile.write(regular_results)\n",
    "        \n",
    "        # test for indomain model\n",
    "        if eval_score_indomain > best_valid_indomain:\n",
    "            best_valid_indomain = eval_score_indomain\n",
    "            torch.save(indomain_model, params['model_dir'] + 'regular_model_moral.pth')\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_indomain = indomain_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_indomain)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_indomain = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_indomain < test_score_indomain:\n",
    "                best_test_indomain = test_score_indomain\n",
    "            indomain_results = 'Test on Indomain RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_indomain, best_valid_indomain)\n",
    "            # print('Regular Results: ', indomain_results)\n",
    "            wfile.write(indomain_results)\n",
    "\n",
    "        if eval_score_adapt > best_valid_adapt:\n",
    "            best_valid_adapt = eval_score_adapt\n",
    "            torch.save(adapt_model, params['model_dir'] + 'adapt_model_moral.pth')\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_adapt = adapt_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_adapt)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_adapt < test_score_adapt:\n",
    "                best_test_adapt = test_score_adapt\n",
    "            test_score_adapt = 'Test on Adapt RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_adapt, best_valid_adapt)\n",
    "            # print('Adapt Results: ', test_score_adapt)\n",
    "            wfile.write(test_score_adapt)\n",
    "            wfile.write('----------------------------------------------------\\n')\n",
    "            \n",
    "    print('Best on Regular RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                domain, best_test_regular, best_valid_regular))\n",
    "    print('Best on InDomain RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                domain, best_test_indomain, best_valid_indomain))\n",
    "    print('Best on Adapt RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                domain, best_test_adapt, best_valid_adapt))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# vaccine experiments\n",
    "vaccine_data = {\n",
    "    'docs': [],\n",
    "    'labels': [],\n",
    "}\n",
    "# wfile.write('\\nVaccine Evaluation---\\n')\n",
    "\n",
    "for idx, row in vaccine_df.iterrows():\n",
    "    encode_label = [0] * params['num_label']\n",
    "    for label_index, _ in enumerate(all_labels):\n",
    "        if np.isnan(np.array(row[all_labels[label_index]], dtype=np.float32)):\n",
    "            continue\n",
    "        if int(row[all_labels[label_index]]) == 1:\n",
    "            encode_label[label_index] = 1\n",
    "    if sum(encode_label) == 0:\n",
    "        encode_label[-1] = 1\n",
    "    vaccine_data['docs'].append(row['text'])\n",
    "    vaccine_data['labels'].append(encode_label)\n",
    "\n",
    "vaccine_train_docs, vaccine_test_docs, vaccine_train_labels, vaccine_test_labels = train_test_split(\n",
    "    vaccine_data['docs'], vaccine_data['labels'], test_size=.50, random_state=33)\n",
    "vaccine_train = {\n",
    "    'docs': [item for item in vaccine_train_docs],\n",
    "    'labels': [item for item in vaccine_train_labels],\n",
    "    'corpus': [1] * len(vaccine_train_docs)\n",
    "}\n",
    "vaccine_test = {\n",
    "    'docs': [item for item in vaccine_data['docs'][250:]],\n",
    "    'labels': [item for item in vaccine_data['labels'][250:]],\n",
    "    'corpus': [1] * len(vaccine_test_docs)\n",
    "}\n",
    "all_train = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': [0] * len(all_data.labels.to_list())\n",
    "}\n",
    "all_train['docs'].extend([item for item in vaccine_train['docs']])\n",
    "all_train['labels'].extend([item for item in vaccine_train['labels']])\n",
    "all_train['corpus'].extend([1] * len(vaccine_train['docs']))\n",
    "\n",
    "all_data_corpus = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': [0] * len(all_data.labels.to_list())\n",
    "}\n",
    "all_data_corpus['docs'].extend([item for item in vaccine_data['docs']])\n",
    "all_data_corpus['labels'].extend([item for item in vaccine_data['labels']])\n",
    "all_data_corpus['corpus'].extend([1] * 500)\n",
    "\n",
    "vaccine_train_data = TorchDataset(vaccine_train, domain_name=params['domain_name'])\n",
    "vaccine_test_data = TorchDataset(vaccine_test, domain_name=params['domain_name'])\n",
    "all_train_data = TorchDataset(all_train, domain_name=params['domain_name'])\n",
    "all_data_torch = TorchDataset(all_data_corpus, domain_name=params['domain_name'])\n",
    "\n",
    "vaccine_train_data_loader = DataLoader(\n",
    "    vaccine_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='rnn')\n",
    ")\n",
    "train_data_loader = DataLoader(\n",
    "    all_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='rnn')\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "    vaccine_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='rnn')\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    vaccine_test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=DataEncoder(params, mtype='rnn')\n",
    ")\n",
    "all_data_loader = DataLoader(\n",
    "    all_data_torch, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='rnn')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust parameters\n",
    "params['epochs'] = 30\n",
    "params['bidirectional'] = True\n",
    "params['lr'] = 9e-4\n",
    "params['emb_dim'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 30, 'batch_size': 64, 'lr': 0.0009, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 300, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': True, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:20<00:00, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Vaccine, F1-micro-average 0.7558735172383356, Valid Score 1.0\n",
      "\n",
      "Best on InDomain RNN, Domain Vaccine, F1-micro-average 0.7780610984885903, Valid Score 0.9889021332566073\n",
      "\n",
      "Best on Adapt RNN, Domain Vaccine, F1-micro-average 0.8197796224363838, Valid Score 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regular_model = RegularRNN(params)\n",
    "regular_model = regular_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "regular_optim = torch.optim.RMSprop(regular_model.parameters(), lr=params['lr'])\n",
    "\n",
    "indomain_model = RegularRNN(params)\n",
    "indomain_model = indomain_model.to(device)\n",
    "indomain_optim = torch.optim.RMSprop(indomain_model.parameters(), lr=params['lr'])\n",
    "\n",
    "adapt_model = AdaptRNN(params)\n",
    "adapt_model = adapt_model.to(device)\n",
    "criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "pred_params = [param for name, param in adapt_model.named_parameters() if 'domain' not in name]\n",
    "adapt_pred_optim = torch.optim.RMSprop(pred_params, lr=params['lr'])\n",
    "domain_params = [param for name, param in adapt_model.named_parameters() if 'domain' in name]\n",
    "adapt_domain_optim = torch.optim.RMSprop(domain_params, lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "best_valid_regular = 0.\n",
    "best_valid_adapt = 0.\n",
    "best_valid_indomain = 0.\n",
    "\n",
    "best_test_regular = 0.\n",
    "best_test_adapt = 0.\n",
    "best_test_indomain = 0.\n",
    "\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "    train_loss_regular = 0.\n",
    "    train_loss_adapt = 0.\n",
    "    adapt_model.train()\n",
    "    regular_model.train()\n",
    "    indomain_model.train()\n",
    "    \n",
    "    # train indomain model for comparison\n",
    "    for step, train_batch in enumerate(vaccine_train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        indomain_optim.zero_grad()\n",
    "        # indomain models\n",
    "        indomain_preds = indomain_model(**{'input_docs': input_docs})\n",
    "        loss = criterion(indomain_preds, input_labels)\n",
    "        loss.backward()\n",
    "        indomain_optim.step()\n",
    "\n",
    "    # train discriminator first\n",
    "    for step, train_batch in enumerate(all_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        adapt_domain_optim.zero_grad()\n",
    "        domain_preds = adapt_model.discriminator(**{'input_docs': input_docs})\n",
    "        domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "        domain_loss.backward()\n",
    "        adapt_domain_optim.step()\n",
    "\n",
    "    # train predictor\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        regular_optim.zero_grad()\n",
    "        adapt_pred_optim.zero_grad()\n",
    "        # adapt_domain_optim.zero_grad()\n",
    "\n",
    "        # regular models\n",
    "        regular_preds = regular_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss = criterion(regular_preds, input_labels)\n",
    "        train_loss_regular += loss.item()\n",
    "        loss_avg_regular = train_loss_regular / (step + 1)\n",
    "\n",
    "        # adapt models\n",
    "        adapt_preds = adapt_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "        domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "        loss_adapt = loss_adapt.mean(axis=1)\n",
    "        loss_adapt = domain_preds[:, 1] * loss_adapt\n",
    "        loss_adapt = loss_adapt.mean()\n",
    "        train_loss_adapt += loss_adapt.item()\n",
    "        loss_avg_adapt = train_loss_adapt / (step + 1)\n",
    "\n",
    "#         if (step + 1) % 301 == 0:\n",
    "#             print('Epoch: {}, Step: {}'.format(epoch, step))\n",
    "#             print('\\tRegular Loss: {}.'.format(loss_avg_regular))\n",
    "#             print('\\tAdapt Loss: {}.'.format(loss_avg_adapt))\n",
    "#             print('-------------------------------------------------')\n",
    "\n",
    "        loss_adapt.backward()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "        regular_optim.step()\n",
    "        adapt_pred_optim.step()\n",
    "\n",
    "    # fit on in domain corpus.\n",
    "    for _ in range(3):\n",
    "        for step, train_batch in enumerate(vaccine_train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            adapt_pred_optim.zero_grad()\n",
    "            adapt_preds = adapt_model(**{'input_docs': input_docs})\n",
    "            loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "            loss_adapt = loss_adapt.mean()\n",
    "            loss_adapt.backward()\n",
    "            adapt_pred_optim.step()\n",
    "\n",
    "    # evaluate on valid data\n",
    "    regular_model.eval()\n",
    "    adapt_model.eval()\n",
    "    indomain_model.eval()\n",
    "    y_preds_regular = []\n",
    "    y_preds_adapt = []\n",
    "    y_preds_indomain = []\n",
    "    y_trues = []\n",
    "\n",
    "    for valid_batch in valid_data_loader:\n",
    "        valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "        input_docs, input_labels, input_domains = valid_batch\n",
    "        with torch.no_grad():\n",
    "            preds_regular = regular_model(**{'input_docs': input_docs})\n",
    "            preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "            preds_indomain = indomain_model(**{'input_docs': input_docs})\n",
    "\n",
    "        logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "        logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "        logits_indomain = (torch.sigmoid(preds_indomain) > .4).long().cpu().numpy()\n",
    "\n",
    "        y_preds_regular.extend(logits_regular)\n",
    "        y_preds_adapt.extend(logits_adapt)\n",
    "        y_preds_indomain.extend(logits_indomain)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "    eval_score_regular = micro_f1_average(y_preds=y_preds_regular, y_truths=y_trues)\n",
    "    eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "    eval_score_indomain = micro_f1_average(y_preds=y_preds_indomain, y_truths=y_trues)\n",
    "\n",
    "    # test for regular model\n",
    "    if eval_score_regular > best_valid_regular:\n",
    "        best_valid_regular = eval_score_regular\n",
    "        torch.save(regular_model, params['model_dir'] + 'regular_rnn_moral.pth')\n",
    "\n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_regular = regular_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_regular)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_regular = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if test_score_regular > best_test_regular:\n",
    "            best_test_regular = test_score_regular\n",
    "        regular_results = 'Test on Regular RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                'vaccine', epoch, test_score_regular, best_valid_regular)\n",
    "#         print('Regular Results: ', regular_results)\n",
    "#         wfile.write(regular_results)\n",
    "\n",
    "    # test for indomain model\n",
    "    if eval_score_indomain > best_valid_indomain:\n",
    "        best_valid_indomain = eval_score_indomain\n",
    "        torch.save(indomain_model, params['model_dir'] + 'indomain_rnn_moral.pth')\n",
    "\n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_indomain = indomain_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_indomain)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_indomain = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if test_score_indomain > best_test_indomain:\n",
    "            best_test_indomain = test_score_indomain\n",
    "        indomain_results = 'Test on Indomain RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                'vaccine', epoch, test_score_indomain, best_valid_indomain)\n",
    "#         print('Regular Results: ', indomain_results)\n",
    "#         wfile.write(indomain_results)\n",
    "\n",
    "    if eval_score_adapt > best_valid_adapt:\n",
    "        best_valid_adapt = eval_score_adapt\n",
    "\n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_adapt = adapt_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_adapt)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if test_score_adapt > best_test_adapt:\n",
    "            best_test_adapt = test_score_adapt\n",
    "            torch.save(adapt_model, params['model_dir'] + 'adapt_rnn_vaccine.pth')\n",
    "        test_score_adapt = 'Test on Adapt RNN, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            'vaccine', epoch, test_score_adapt, best_valid_adapt)\n",
    "#         print('Adapt Results: ', test_score_adapt)\n",
    "#         wfile.write(test_score_adapt)\n",
    "\n",
    "# wfile.write('\\n\\n\\n')\n",
    "# wfile.close()\n",
    "domain = 'Vaccine'\n",
    "print('Best on Regular RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_regular, best_valid_regular))\n",
    "print('Best on InDomain RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_indomain, best_valid_indomain))\n",
    "print('Best on Adapt RNN, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_adapt, best_valid_adapt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_rnn.txt', 'model_dir': '../resource/model/adapt_rnn/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 50, 'batch_size': 64, 'lr': 0.0009, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'rmsprop', 'emb_path': '/data/models/glove.twitter.27B.200d.txt', 'emb_dim': 300, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'num_label': 11, 'tok_dir': '../resource/model/adapt_rnn', 'word_emb_path': '../resource/model/adapt_rnn/all.npy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:38<00:00,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Sandy, F1-micro-average 0.780741671813914, Valid Score 1.0\n",
      "\n",
      "Best on InDomain RNN, Domain Sandy, F1-micro-average 0.7659039209845813, Valid Score 0.9979959919839679\n",
      "\n",
      "Best on Adapt RNN, Domain Sandy, F1-micro-average 0.8153037593267491, Valid Score 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "# evaluate on the test set\n",
    "for test_batch in test_data_loader:\n",
    "    test_batch = tuple(t.to(device) for t in test_batch)\n",
    "    input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_adapt = adapt_model(**{\n",
    "            'input_docs': input_docs,\n",
    "        })\n",
    "    logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "    y_preds.extend(logits_adapt)\n",
    "    y_trues.extend(input_labels.to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues_bin = [0 if item[-1] > 0 else 1 for item in y_trues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_bin = [0 if item[-1] > 0 else 1 for item in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_true=y_trues_bin, y_pred=y_preds_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363661016949153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true=y_trues_bin, y_pred=y_preds_bin, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
