{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import gensim\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "spw_set = set(stopwords.words('english'))\n",
    "spw_set.add('url')\n",
    "tokenizer = TweetTokenizer()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# set the which GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess a single tweet\n",
    "    :param tweet:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global tokenizer\n",
    "\n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # noinspection PyUnresolvedReferences\n",
    "    tweet = re.sub(r\"https?:\\S+\", \"URL\", tweet)  # replace url\n",
    "    # replace user\n",
    "    # tweet = re.sub(r'@\\w+', 'USER', tweet)\n",
    "    # replace hashtag\n",
    "    # tweet = re.sub(r'#\\S+', 'HASHTAG', tweet)\n",
    "    # tokenize\n",
    "    return [item.strip() for item in tokenizer.tokenize(tweet) if len(item.strip()) > 0]\n",
    "\n",
    "\n",
    "def label_encoder(raw_label):\n",
    "    pre_labels = [\n",
    "        'subversion', 'loyalty', 'care', 'cheating',\n",
    "        'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "    ]\n",
    "    encode_label = [0]*(len(pre_labels) + 1)\n",
    "    if type(raw_label) != str:\n",
    "        encode_label[-1] = 1\n",
    "        return encode_label\n",
    "    for label in raw_label.split(','):\n",
    "        if label not in pre_labels:\n",
    "            encode_label[-1] = 1\n",
    "        else:\n",
    "            encode_label[pre_labels.index(label)] = 1\n",
    "    return encode_label\n",
    "\n",
    "\n",
    "def micro_f1_average(y_preds, y_truths):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for idx, (y_pred, y_truth) in enumerate(zip(y_preds, y_truths)):\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        true_positives = np.sum(np.logical_and(y_truth, y_pred))\n",
    "\n",
    "        # compute the sum of tp + fp across training examples and labels\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        l_prec_den = np.sum(y_pred)\n",
    "        if l_prec_den != 0:\n",
    "            # compute micro-averaged precision\n",
    "            precisions.append(true_positives / l_prec_den)\n",
    "\n",
    "        # compute sum of tp + fn across training examples and labels\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        l_recall_den = np.sum(y_truth)\n",
    "\n",
    "        # compute mirco-average recall\n",
    "        if l_recall_den != 0:\n",
    "            recalls.append(true_positives / l_recall_den)\n",
    "\n",
    "    precisions = np.average(precisions)\n",
    "    recalls = np.average(recalls)\n",
    "    if precisions + recalls == 0:\n",
    "        return 0\n",
    "    f1 = 2 * precisions * recalls / (precisions + recalls)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def multi_label_f1(y_preds, y_truths, mode='weighted'):\n",
    "    preds = dict()\n",
    "    truths = dict()\n",
    "    for idx in range(len(y_truths)):\n",
    "        for jdx in range(len(y_truths[idx])):\n",
    "            if jdx not in preds:\n",
    "                preds[jdx] = []\n",
    "                truths[jdx] = []\n",
    "            preds[jdx].append(y_preds[idx][jdx])\n",
    "            truths[jdx].append(y_truths[idx][jdx])\n",
    "    results = []\n",
    "    for jdx in preds:\n",
    "        results.append(metrics.f1_score(preds[jdx], truths[jdx], average=mode))\n",
    "    return np.average(results)\n",
    "\n",
    "\n",
    "def build_wt(tkn, emb_path, opath):\n",
    "    \"\"\"Build weight using word embedding\"\"\"\n",
    "    embed_len = len(tkn.word_index)\n",
    "    if embed_len > tkn.num_words:\n",
    "        embed_len = tkn.num_words\n",
    "\n",
    "    if emb_path.endswith('.bin'):\n",
    "        embeds = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            emb_path, binary=True, unicode_errors='ignore'\n",
    "        )\n",
    "        emb_size = embeds.vector_size\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        for pair in zip(embeds.wv.index2word, embeds.wv.syn0):\n",
    "            if pair[0] in tkn.word_index and \\\n",
    "                    tkn.word_index[pair[0]] < tkn.num_words:\n",
    "                emb_matrix[tkn.word_index[pair[0]]] = np.asarray([\n",
    "                    float(item) for item in pair[1]\n",
    "                ], dtype=np.float32)\n",
    "    else:\n",
    "        dfile = open(emb_path)\n",
    "        line = dfile.readline().strip().split()\n",
    "        if len(line) < 5:\n",
    "            line = dfile.readline().strip().split()\n",
    "        emb_size = len(line[1:])\n",
    "        emb_matrix = list(np.zeros((embed_len + 1, emb_size)))\n",
    "        dfile.close()\n",
    "\n",
    "        with open(emb_path) as dfile:\n",
    "            for line in dfile:\n",
    "                line = line.strip().split()\n",
    "                if line[0] in tkn.word_index and \\\n",
    "                        tkn.word_index[line[0]] < tkn.num_words:\n",
    "                    emb_matrix[tkn.word_index[line[0]]] = np.asarray([\n",
    "                        float(item) for item in line[1:]\n",
    "                    ], dtype=np.float32)\n",
    "    # emb_matrix = np.array(emb_matrix, dtype=np.float32)\n",
    "    np.save(opath, emb_matrix)\n",
    "    return emb_matrix\n",
    "\n",
    "\n",
    "def build_tok(docs, max_feature, opath):\n",
    "    if os.path.exists(opath):\n",
    "        return pickle.load(open(opath, 'rb'))\n",
    "    else:\n",
    "        # load corpus\n",
    "        tkn = Tokenizer(num_words=max_feature)\n",
    "        tkn.fit_on_texts(docs)\n",
    "\n",
    "        with open(opath, 'wb') as wfile:\n",
    "            pickle.dump(tkn, wfile)\n",
    "        return tkn\n",
    "\n",
    "\n",
    "class DataEncoder(object):\n",
    "    def __init__(self, params, mtype='rnn'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param params:\n",
    "        :param mtype: Model type, rnn or bert\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.mtype = mtype\n",
    "        if self.mtype == 'rnn':\n",
    "            self.tok = pickle.load(open(\n",
    "                os.path.join(params['tok_dir'], '{}.tok'.format(params['dname'])), 'rb'))\n",
    "        elif self.mtype == 'bert':\n",
    "            self.tok = AutoTokenizer.from_pretrained(params['bert_name'], )\n",
    "        else:\n",
    "            raise ValueError('Only support BERT and RNN data encoders')\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        docs = []\n",
    "        labels = []\n",
    "        domains = []\n",
    "        for text, label, domain in batch:\n",
    "            if self.mtype == 'bert':\n",
    "                text = self.tok.encode_plus(\n",
    "                    text, padding='max_length', max_length=self.params['max_len'],\n",
    "                    return_tensors='pt', return_token_type_ids=False,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                docs.append(text['input_ids'][0])\n",
    "            else:\n",
    "                docs.append(text)\n",
    "            labels.append(label)\n",
    "            domains.append(domain)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        domains = torch.tensor(domains, dtype=torch.long)\n",
    "        if self.mtype == 'rnn':\n",
    "            # padding and tokenize\n",
    "            docs = self.tok.texts_to_sequences(docs)\n",
    "            docs = pad_sequences(docs)\n",
    "            docs = torch.Tensor(docs).long()\n",
    "        else:\n",
    "            docs = torch.stack(docs).long()\n",
    "        return docs, labels, domains\n",
    "\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, dataset, domain_name):\n",
    "        self.dataset = dataset\n",
    "        self.domain_name = domain_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['docs'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.domain_name in self.dataset:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], self.dataset[self.domain_name][idx]\n",
    "        else:\n",
    "            return self.dataset['docs'][idx], self.dataset['labels'][idx], -1\n",
    "\n",
    "\n",
    "class RegularBERT(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RegularBERT, self).__init__()\n",
    "        self.params = params\n",
    "\n",
    "        self.bert_model = AutoModel.from_pretrained(self.params['bert_name'])\n",
    "        self.dropout = nn.Dropout(self.params['dp_rate'])\n",
    "        \n",
    "        # gru layer\n",
    "#         self.doc_net_general = nn.GRU(\n",
    "#             self.wemb.embedding_dim, self.word_hidden_size,\n",
    "#             bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        # prediction\n",
    "        self.predictor = nn.Linear(\n",
    "            self.bert_model.config.hidden_size, self.params['num_label'], bias=False)\n",
    "\n",
    "    def forward(self, input_docs):\n",
    "        output_bert = self.bert_model(input_docs)\n",
    "        # take the outputs pooler layer\n",
    "        # doc_embs = self.linear(output_bert[1])\n",
    "        doc_embs = torch.mean(output_bert[0], dim=1)\n",
    "        doc_embs = torch.squeeze(doc_embs)\n",
    "        doc_embs = self.dropout(doc_embs)\n",
    "        # doc_embs = torch.relu(self.linear(doc_embs))\n",
    "\n",
    "        # prediction\n",
    "        doc_preds = self.predictor(doc_embs)\n",
    "        return doc_preds\n",
    "\n",
    "\n",
    "class AdaptBERT(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(AdaptBERT, self).__init__()\n",
    "        self.params = params\n",
    "        self.bert_model = AutoModel.from_pretrained(self.params['bert_name'])\n",
    "        self.dropout = nn.Dropout(self.params['dp_rate'])\n",
    "\n",
    "        # domain prediction\n",
    "#         self.domain_net = nn.GRU(\n",
    "#             self.wemb.embedding_dim, self.word_hidden_size,\n",
    "#             bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        # two domains, this domain vs others\n",
    "        self.domain_clf = nn.Linear(\n",
    "            self.bert_model.config.hidden_size, 2, bias=False\n",
    "        )\n",
    "\n",
    "        # regular prediction\n",
    "#         self.document_net = nn.GRU(\n",
    "#             self.wemb.embedding_dim, self.word_hidden_size,\n",
    "#             bidirectional=self.params['bidirectional'], dropout=self.params['dp_rate'],\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        # prediction\n",
    "        self.document_predictor = nn.Linear(\n",
    "            self.bert_model.config.hidden_size, self.params['num_label'], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input_docs):\n",
    "        output_bert = self.bert_model(input_docs)\n",
    "        # take the outputs pooler layer\n",
    "        # doc_embs = self.linear(output_bert[1])\n",
    "        doc_embs = torch.mean(output_bert[0], dim=1)\n",
    "        doc_embs = torch.squeeze(doc_embs)\n",
    "        doc_embs = self.dropout(doc_embs)\n",
    "        \n",
    "        # prediction\n",
    "        doc_preds = self.document_predictor(doc_embs)\n",
    "        return doc_preds\n",
    "\n",
    "    def discriminator(self, input_docs):\n",
    "        output_bert = self.bert_model(input_docs)\n",
    "        # take the outputs pooler layer\n",
    "        # doc_embs = self.linear(output_bert[1])\n",
    "        doc_embs = torch.mean(output_bert[0], dim=1)\n",
    "        doc_embs = torch.squeeze(doc_embs)\n",
    "        doc_embs = self.dropout(doc_embs)\n",
    "\n",
    "        # prediction\n",
    "        domain_preds = self.domain_clf(doc_embs)\n",
    "        return domain_preds\n",
    "\n",
    "    def freeze_layer(self, if_train=True):\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = if_train\n",
    "\n",
    "\n",
    "def data_split(data):\n",
    "    \"\"\"\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_indices = list(range(len(data['docs'])))\n",
    "    np.random.seed(33)  # for reproductive results\n",
    "    np.random.shuffle(data_indices)\n",
    "\n",
    "    train_indices = data_indices[:int(.8 * len(data_indices))]\n",
    "    dev_indices = data_indices[int(.8 * len(data_indices)):int(.9 * len(data_indices))]\n",
    "    test_indices = data_indices[int(.9 * len(data_indices)):]\n",
    "    return train_indices, dev_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_morality = [\n",
    "    'subversion', 'loyalty', 'care', 'cheating',\n",
    "    'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "]\n",
    "\n",
    "result_dir = '../resource/results/'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "model_dir = '../resource/model/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "model_dir = model_dir + 'adapt_bert/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "params = {\n",
    "    'result_path': os.path.join(result_dir, 'adapt_bert.txt'),\n",
    "    'model_dir': model_dir,\n",
    "    'dname': 'all',\n",
    "    'dpath': '../data/dataset.tsv',\n",
    "    'max_feature': 15000,\n",
    "    'over_sample': True,\n",
    "    'domain_name': 'corpus',\n",
    "    'epochs': 10,\n",
    "    'batch_size': 16,\n",
    "    'lr': 9e-5,\n",
    "    'max_len': 60,\n",
    "    'dp_rate': .2,\n",
    "    'optimizer': 'adam',\n",
    "    'emb_dim': 200,\n",
    "    'unique_domains': [],\n",
    "    'bidirectional': False,\n",
    "    'device': 'cuda',\n",
    "    'bert_name': 'bert-base-uncased', # 'bert-base-uncased','vinai/bertweet-base', 'digitalepidemiologylab/covid-twitter-bert'\n",
    "    'num_label': len(all_morality)+1,  # plus no-moral\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    }
   ],
   "source": [
    "all_labels = [\n",
    "    'subversion', 'loyalty', 'care', 'cheating',\n",
    "    'purity', 'fairness', 'degradation', 'betrayal', 'harm', 'authority'\n",
    "]\n",
    "wfile = open(params['result_path'], 'a')\n",
    "\n",
    "print('Loading Data...')\n",
    "all_data = pd.read_csv(params['dpath'], sep='\\t', dtype=str)\n",
    "all_data.tid = all_data.tid.apply(lambda x: str(x))\n",
    "all_data = all_data[~all_data.text.isna()]\n",
    "all_data = all_data[~all_data.labels.isna()]\n",
    "# preprocess tweet and remove short tweet\n",
    "all_data.text = all_data.text.apply(lambda x: preprocess(x))\n",
    "all_data = all_data[all_data.text.apply(lambda x: len(x) > 3)]\n",
    "all_data.text = all_data.text.apply(lambda x: ' '.join(x))\n",
    "all_data.labels = all_data.labels.apply(lambda x: label_encoder(x))\n",
    "params['unique_domains'] = list(all_data.corpus.unique())\n",
    "wfile.write(json.dumps(params) + '\\n')\n",
    "\n",
    "if torch.cuda.is_available() and params['device'] != 'cpu':\n",
    "    device = torch.device(params['device'])\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "params['device'] = device\n",
    "\n",
    "# load the vaccine data and test the classifier on the vaccine data\n",
    "vaccine_df = pd.read_csv('../data/vaccine_morality.csv', dtype=str)\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: preprocess(x))\n",
    "# vaccine_df = vaccine_df[vaccine_df.text.apply(lambda x: len(x) > 3)]\n",
    "vaccine_df.text = vaccine_df.text.apply(lambda x: ' '.join(x))\n",
    "vaccine_df = vaccine_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# domains\n",
    "domain_encoder = list(all_data.corpus.unique()) + ['vaccine']\n",
    "\n",
    "# use half of the vaccine as train and half as test\n",
    "all_corpus = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': all_data.corpus.to_list(),\n",
    "}\n",
    "all_corpus['corpus'] = [domain_encoder.index(item) for item in all_corpus['corpus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lr'] = 9e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run over domains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_bert.txt', 'model_dir': '../resource/model/adapt_bert/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 10, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'adam', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'bert_name': 'vinai/bertweet-base', 'num_label': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [03:48<34:19, 228.79s/it]\u001b[A\n",
      " 20%|██        | 2/10 [07:33<30:20, 227.59s/it]\u001b[A\n",
      " 30%|███       | 3/10 [11:17<26:25, 226.54s/it]\u001b[A\n",
      " 40%|████      | 4/10 [15:03<22:37, 226.32s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [18:47<18:47, 225.58s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [22:31<15:00, 225.11s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [26:15<11:14, 224.83s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [29:59<07:29, 224.61s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [33:43<03:44, 224.49s/it]\u001b[A\n",
      "100%|██████████| 10/10 [37:28<00:00, 224.82s/it]\u001b[A\n",
      " 29%|██▊       | 2/7 [37:36<1:34:00, 1128.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Adapt BERT, Domain Baltimore, F1-micro-average 0.6443542341900416, Valid Score 0.6408787997977187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_bert.txt', 'model_dir': '../resource/model/adapt_bert/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 10, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'adam', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'bert_name': 'vinai/bertweet-base', 'num_label': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [03:48<34:15, 228.42s/it]\u001b[A\n",
      " 20%|██        | 2/10 [07:32<30:17, 227.15s/it]\u001b[A\n",
      " 30%|███       | 3/10 [11:16<26:22, 226.08s/it]\u001b[A\n",
      " 40%|████      | 4/10 [15:00<22:32, 225.47s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [18:44<18:45, 225.05s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [22:28<14:58, 224.70s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [26:11<11:13, 224.36s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [29:55<07:28, 224.11s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [33:39<03:44, 224.13s/it]\u001b[A\n",
      "100%|██████████| 10/10 [37:23<00:00, 224.35s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [1:15:07<1:37:39, 1464.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Adapt BERT, Domain BLM, F1-micro-average 0.0, Valid Score 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_bert.txt', 'model_dir': '../resource/model/adapt_bert/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 10, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'adam', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'bert_name': 'vinai/bertweet-base', 'num_label': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [03:49<34:22, 229.14s/it]\u001b[A\n",
      " 20%|██        | 2/10 [07:33<30:22, 227.80s/it]\u001b[A\n",
      " 30%|███       | 3/10 [12:56<30:11, 258.77s/it]\u001b[A\n",
      " 43%|████▎     | 3/7 [1:28:11<1:57:35, 1763.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4d23756f68c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 })\n\u001b[1;32m    118\u001b[0m                 \u001b[0mdomain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_domains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mdomain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0madapt_domain_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m#         sys.exit(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Run over domains...')\n",
    "for didx, domain in enumerate(tqdm(params['unique_domains'])):\n",
    "    if domain in ['ALM', ]: #  'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo'\n",
    "        continue\n",
    "    wfile.write('Working on Domain {}, Domain index {} \\n'.format(domain, didx))\n",
    "    in_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] == didx]\n",
    "    out_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] != didx]\n",
    "\n",
    "    train_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in out_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in out_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in out_domain_indices],\n",
    "    }\n",
    "    domain_corpus = {\n",
    "        'docs': [item for item in train_corpus['docs']],\n",
    "        'labels': [item for item in train_corpus['labels']],\n",
    "        'corpus': [0] * len(train_corpus['docs']),  # first collect documents from out of domain\n",
    "    }\n",
    "    in_domain_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in in_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in in_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in in_domain_indices],\n",
    "    }\n",
    "    \n",
    "    domain_corpus['docs'].extend(in_domain_corpus['docs'])\n",
    "    domain_corpus['labels'].extend(in_domain_corpus['labels'])\n",
    "    domain_corpus['corpus'].extend([1] * len(in_domain_corpus['docs']))\n",
    "\n",
    "    # 10% for training, 10% for valid, the rest for testing\n",
    "    test_indices, val_indices, train_indices = data_split(in_domain_corpus)\n",
    "    in_domain_train = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in train_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in train_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in train_indices]\n",
    "    }\n",
    "    train_corpus['docs'].extend(in_domain_train['docs'])\n",
    "    train_corpus['labels'].extend(in_domain_train['labels'])\n",
    "    train_corpus['corpus'].extend(in_domain_train['corpus'])\n",
    "\n",
    "    valid_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in val_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in val_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in val_indices]\n",
    "    }\n",
    "    test_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in test_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in test_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in test_indices]\n",
    "    }\n",
    "\n",
    "    # start to iteratively train and test the proposed approach.\n",
    "    train_data = TorchDataset(train_corpus, params['domain_name'])\n",
    "    valid_data = TorchDataset(valid_corpus, params['domain_name'])\n",
    "    test_data = TorchDataset(test_corpus, params['domain_name'])\n",
    "    in_domain_train_data = TorchDataset(in_domain_train, params['domain_name'])\n",
    "    domain_data = TorchDataset(domain_corpus, params['domain_name'])\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    test_data_loader = DataLoader(\n",
    "        test_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    in_domain_train_data_loader = DataLoader(\n",
    "        in_domain_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    domain_data_loader = DataLoader(\n",
    "        domain_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "\n",
    "    omit_optim_names = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "    adapt_model = AdaptBERT(params)\n",
    "    adapt_model = adapt_model.to(device)\n",
    "    domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "#     pred_params = [\n",
    "#         param for name, param in adapt_model.named_parameters() if 'domain' not in name and not any(nd in name for nd in omit_optim_names)]\n",
    "#     adapt_pred_optim = torch.optim.Adam(pred_params, lr=params['lr']*5)\n",
    "    pred_params = [\n",
    "        param for name, param in adapt_model.named_parameters() if 'domain' not in name]\n",
    "    adapt_pred_optim = torch.optim.Adam(pred_params, lr=params['lr'])\n",
    "#     domain_params = [\n",
    "#         param for name, param in adapt_model.named_parameters() if ('domain' in name or 'bert' in name) and not any(nd in name for nd in omit_optim_names)]\n",
    "#     adapt_domain_optim = torch.optim.Adam(domain_params, lr=params['lr'])\n",
    "    domain_params = [\n",
    "        param for name, param in adapt_model.named_parameters() if 'domain' in name]\n",
    "    adapt_domain_optim = torch.optim.Adam(domain_params, lr=params['lr']*5)\n",
    "\n",
    "    # train the networks\n",
    "    print('Start to train...')\n",
    "    print(params)\n",
    "    best_valid_adapt = 0.    \n",
    "    best_test_adapt = 0.\n",
    "\n",
    "    for epoch in tqdm(range(params['epochs'])):\n",
    "        train_loss_adapt = 0.\n",
    "        adapt_model.train()\n",
    "            \n",
    "        # train discriminator first\n",
    "        # adapt_model.freeze_layer(False)\n",
    "        for _ in range(3):\n",
    "            for step, train_batch in enumerate(domain_data_loader):\n",
    "                train_batch = tuple(t.to(device) for t in train_batch)\n",
    "                input_docs, input_labels, input_domains = train_batch\n",
    "                adapt_domain_optim.zero_grad()\n",
    "                domain_preds = adapt_model.discriminator(**{\n",
    "                    'input_docs': input_docs\n",
    "                })\n",
    "                domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "                domain_loss.backward()\n",
    "                adapt_domain_optim.step()\n",
    "#         sys.exit(-1)\n",
    "        # train predictor\n",
    "        # adapt_model.freeze_layer(True)\n",
    "        for step, train_batch in enumerate(train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            if len(input_docs) == 1:\n",
    "                continue\n",
    "            adapt_pred_optim.zero_grad()\n",
    "\n",
    "            # adapt models\n",
    "            adapt_preds = adapt_model(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "            domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "            loss_adapt = loss_adapt.mean(axis=1)\n",
    "#             loss_adapt = domain_preds[:, 1] * loss_adapt\n",
    "            loss_adapt = loss_adapt.mean()\n",
    "            train_loss_adapt += loss_adapt.item()\n",
    "            loss_avg_adapt = train_loss_adapt / (step + 1)\n",
    "\n",
    "            loss_adapt.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "            adapt_pred_optim.step()\n",
    "\n",
    "        # fit on in domain corpus.\n",
    "        for _ in range(3):\n",
    "            for step, train_batch in enumerate(in_domain_train_data_loader):\n",
    "                train_batch = tuple(t.to(device) for t in train_batch)\n",
    "                input_docs, input_labels, input_domains = train_batch\n",
    "                if len(input_docs) == 1:\n",
    "                    continue\n",
    "                adapt_pred_optim.zero_grad()\n",
    "                adapt_preds = adapt_model(**{\n",
    "                    'input_docs': input_docs\n",
    "                })\n",
    "                loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "                loss_adapt = loss_adapt.mean()\n",
    "                loss_adapt.backward()\n",
    "                adapt_pred_optim.step()\n",
    "        \n",
    "        # evaluate on valid data\n",
    "        adapt_model.eval()\n",
    "        y_preds_adapt = []\n",
    "        y_trues = []\n",
    "        for valid_batch in valid_data_loader:\n",
    "            valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "            input_docs, input_labels, input_domains = valid_batch\n",
    "            with torch.no_grad():\n",
    "                preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "\n",
    "            logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "            y_preds_adapt.extend(logits_adapt)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "\n",
    "        if eval_score_adapt > best_valid_adapt:\n",
    "            best_valid_adapt = eval_score_adapt\n",
    "            \n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_adapt = adapt_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_adapt)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if best_test_adapt < test_score_adapt:\n",
    "            best_test_adapt = test_score_adapt\n",
    "            torch.save(adapt_model, params['model_dir'] + 'adapt_bert_moral.pth')\n",
    "        test_score_adapt = 'Test on Adapt BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                domain, epoch, test_score_adapt, best_valid_adapt)\n",
    "#         print('Adapt Results: ', test_score_adapt)\n",
    "#         wfile.write(test_score_adapt)\n",
    "    print('Best on Adapt BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "        domain, best_test_adapt, best_valid_adapt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Run over domains...')\n",
    "for didx, domain in enumerate(tqdm(params['unique_domains'])):\n",
    "    if domain in ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo']:\n",
    "        continue\n",
    "    wfile.write('Working on Domain {}, Domain index {} \\n'.format(domain, didx))\n",
    "    in_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] == didx]\n",
    "    out_domain_indices = [item for item in range(len(all_corpus['corpus'])) if all_corpus['corpus'][item] != didx]\n",
    "\n",
    "    train_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in out_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in out_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in out_domain_indices],\n",
    "    }\n",
    "    domain_corpus = {\n",
    "        'docs': [item for item in train_corpus['docs']],\n",
    "        'labels': [item for item in train_corpus['labels']],\n",
    "        'corpus': [0] * len(train_corpus['docs']),  # first collect documents from out of domain\n",
    "    }\n",
    "    in_domain_corpus = {\n",
    "        'docs': [all_corpus['docs'][item] for item in in_domain_indices],\n",
    "        'labels': [all_corpus['labels'][item] for item in in_domain_indices],\n",
    "        'corpus': [all_corpus['corpus'][item] for item in in_domain_indices],\n",
    "    }\n",
    "    \n",
    "    domain_corpus['docs'].extend(in_domain_corpus['docs'])\n",
    "    domain_corpus['labels'].extend(in_domain_corpus['labels'])\n",
    "    domain_corpus['corpus'].extend([1] * len(in_domain_corpus['docs']))\n",
    "\n",
    "    # 10% for training, 10% for valid, the rest for testing\n",
    "    test_indices, val_indices, train_indices = data_split(in_domain_corpus)\n",
    "    in_domain_train = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in train_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in train_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in train_indices]\n",
    "    }\n",
    "    train_corpus['docs'].extend(in_domain_train['docs'])\n",
    "    train_corpus['labels'].extend(in_domain_train['labels'])\n",
    "    train_corpus['corpus'].extend(in_domain_train['corpus'])\n",
    "\n",
    "    valid_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in val_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in val_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in val_indices]\n",
    "    }\n",
    "    test_corpus = {\n",
    "        'docs': [in_domain_corpus['docs'][item] for item in test_indices],\n",
    "        'labels': [in_domain_corpus['labels'][item] for item in test_indices],\n",
    "        'corpus': [in_domain_corpus['corpus'][item] for item in test_indices]\n",
    "    }\n",
    "\n",
    "    # start to iteratively train and test the proposed approach.\n",
    "    train_data = TorchDataset(train_corpus, params['domain_name'])\n",
    "    valid_data = TorchDataset(valid_corpus, params['domain_name'])\n",
    "    test_data = TorchDataset(test_corpus, params['domain_name'])\n",
    "    in_domain_train_data = TorchDataset(in_domain_train, params['domain_name'])\n",
    "    domain_data = TorchDataset(domain_corpus, params['domain_name'])\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    test_data_loader = DataLoader(\n",
    "        test_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    in_domain_train_data_loader = DataLoader(\n",
    "        in_domain_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "    domain_data_loader = DataLoader(\n",
    "        domain_data, batch_size=params['batch_size'], shuffle=True,\n",
    "        collate_fn=DataEncoder(params, mtype='bert')\n",
    "    )\n",
    "\n",
    "    omit_optim_names = ['bias', 'LayerNorm.weight']\n",
    "    regular_model = RegularBERT(params)\n",
    "    regular_model = regular_model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    regular_optim = torch.optim.Adam(\n",
    "        [p for n, p in regular_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "        lr=params['lr']\n",
    "    )\n",
    "    \n",
    "    indomain_model = RegularBERT(params)\n",
    "    indomain_model = indomain_model.to(device)\n",
    "    indomain_optim = torch.optim.Adam(\n",
    "        [p for n, p in indomain_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "        lr=params['lr']\n",
    "    )\n",
    "\n",
    "    adapt_model = AdaptBERT(params)\n",
    "    adapt_model = adapt_model.to(device)\n",
    "    domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "    pred_params = [\n",
    "        param for name, param in adapt_model.named_parameters() if 'domain' not in name and not any(nd in name for nd in omit_optim_names)]\n",
    "    adapt_pred_optim = torch.optim.Adam(pred_params, lr=params['lr'])\n",
    "    domain_params = [\n",
    "        param for name, param in adapt_model.named_parameters() if 'domain' in name and not any(nd in name for nd in omit_optim_names)]\n",
    "    adapt_domain_optim = torch.optim.Adam(domain_params, lr=params['lr'] * 0.1)\n",
    "\n",
    "    # train the networks\n",
    "    print('Start to train...')\n",
    "    print(params)\n",
    "    best_valid_regular = 0.\n",
    "    best_valid_adapt = 0.\n",
    "    best_valid_indomain = 0.\n",
    "    \n",
    "    best_test_regular = 0.\n",
    "    best_test_adapt = 0.\n",
    "    best_test_indomain = 0.\n",
    "\n",
    "    for epoch in tqdm(range(params['epochs'])):\n",
    "        train_loss_regular = 0.\n",
    "        train_loss_adapt = 0.\n",
    "        adapt_model.train()\n",
    "        regular_model.train()\n",
    "        indomain_model.train()\n",
    "        \n",
    "        # train indomain model for comparison\n",
    "        for step, train_batch in enumerate(in_domain_train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            if len(input_docs) == 1:\n",
    "                continue\n",
    "            indomain_optim.zero_grad()\n",
    "            \n",
    "            # indomain models\n",
    "            indomain_preds = indomain_model(**{'input_docs': input_docs})\n",
    "            loss = criterion(indomain_preds, input_labels)\n",
    "            loss.backward()\n",
    "            indomain_optim.step()\n",
    "            \n",
    "        # train discriminator first\n",
    "        # adapt_model.freeze_layer(False)\n",
    "        for step, train_batch in enumerate(domain_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            adapt_domain_optim.zero_grad()\n",
    "            domain_preds = adapt_model.discriminator(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "            domain_loss.backward()\n",
    "            adapt_domain_optim.step()\n",
    "\n",
    "        # train predictor\n",
    "        # adapt_model.freeze_layer(True)\n",
    "        for step, train_batch in enumerate(train_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            if len(input_docs) == 1:\n",
    "                continue\n",
    "            regular_optim.zero_grad()\n",
    "            adapt_pred_optim.zero_grad()\n",
    "            # adapt_domain_optim.zero_grad()\n",
    "\n",
    "            # regular models\n",
    "            regular_preds = regular_model(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            loss = criterion(regular_preds, input_labels)\n",
    "            train_loss_regular += loss.item()\n",
    "            loss_avg_regular = train_loss_regular / (step + 1)\n",
    "\n",
    "            # adapt models\n",
    "            adapt_preds = adapt_model(**{\n",
    "                'input_docs': input_docs\n",
    "            })\n",
    "            loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "            domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "            loss_adapt = loss_adapt.mean(axis=1)\n",
    "            loss_adapt = domain_preds[:, 1] * loss_adapt\n",
    "            loss_adapt = loss_adapt.mean()\n",
    "            train_loss_adapt += loss_adapt.item()\n",
    "            loss_avg_adapt = train_loss_adapt / (step + 1)\n",
    "\n",
    "            loss_adapt.backward()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "            regular_optim.step()\n",
    "            adapt_pred_optim.step()\n",
    "\n",
    "        # fit on in domain corpus.\n",
    "        for _ in range(3):\n",
    "            for step, train_batch in enumerate(in_domain_train_data_loader):\n",
    "                train_batch = tuple(t.to(device) for t in train_batch)\n",
    "                input_docs, input_labels, input_domains = train_batch\n",
    "                if len(input_docs) == 1:\n",
    "                    continue\n",
    "                adapt_pred_optim.zero_grad()\n",
    "                adapt_preds = adapt_model(**{\n",
    "                    'input_docs': input_docs\n",
    "                })\n",
    "                loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "                loss_adapt = loss_adapt.mean()\n",
    "                loss_adapt.backward()\n",
    "                adapt_pred_optim.step()\n",
    "        \n",
    "        # evaluate on valid data\n",
    "        regular_model.eval()\n",
    "        adapt_model.eval()\n",
    "        indomain_model.eval()\n",
    "        y_preds_regular = []\n",
    "        y_preds_adapt = []\n",
    "        y_preds_indomain = []\n",
    "        y_trues = []\n",
    "        for valid_batch in valid_data_loader:\n",
    "            valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "            input_docs, input_labels, input_domains = valid_batch\n",
    "            with torch.no_grad():\n",
    "                preds_regular = regular_model(**{'input_docs': input_docs})\n",
    "                preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "                preds_indomain = indomain_model(**{'input_docs': input_docs})\n",
    "\n",
    "            logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "            logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "            logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "            \n",
    "            y_preds_regular.extend(logits_regular)\n",
    "            y_preds_adapt.extend(logits_adapt)\n",
    "            y_preds_indomain.extend(logits_indomain)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        eval_score_regular = micro_f1_average(y_preds=y_preds_regular, y_truths=y_trues)\n",
    "        eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "        eval_score_indomain = micro_f1_average(y_preds=y_preds_indomain, y_truths=y_trues)\n",
    "\n",
    "        # test for regular model\n",
    "        if eval_score_regular > best_valid_regular:\n",
    "            best_valid_regular = eval_score_regular\n",
    "            torch.save(regular_model, params['model_dir'] + 'regular_bert_moral.pth')\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_regular = regular_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_regular)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_regular = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_regular < test_score_regular:\n",
    "                best_test_regular = test_score_regular\n",
    "            regular_results = 'Test on Regular BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_regular, best_valid_regular)\n",
    "#             print('Regular Results: ', regular_results)\n",
    "#             wfile.write(regular_results)\n",
    "        \n",
    "        # test for indomain model\n",
    "        if eval_score_indomain > best_valid_indomain:\n",
    "            best_valid_indomain = eval_score_indomain\n",
    "            torch.save(indomain_model, params['model_dir'] + 'regular_bert_moral.pth')\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_indomain = indomain_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_indomain)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_indomain = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_indomain < test_score_indomain:\n",
    "                best_test_indomain = test_score_indomain\n",
    "            indomain_results = 'Test on Indomain BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_indomain, best_valid_indomain)\n",
    "#             print('Regular Results: ', indomain_results)\n",
    "#             wfile.write(indomain_results)\n",
    "\n",
    "        if eval_score_adapt > best_valid_adapt:\n",
    "            best_valid_adapt = eval_score_adapt\n",
    "\n",
    "            # test\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            # evaluate on the test set\n",
    "            for test_batch in test_data_loader:\n",
    "                test_batch = tuple(t.to(device) for t in test_batch)\n",
    "                input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    preds_adapt = adapt_model(**{\n",
    "                        'input_docs': input_docs,\n",
    "                    })\n",
    "                logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "                y_preds.extend(logits_adapt)\n",
    "                y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "            test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "            if best_test_adapt < test_score_adapt:\n",
    "                best_test_adapt = test_score_adapt\n",
    "                torch.save(adapt_model, params['model_dir'] + 'adapt_bert_moral.pth')\n",
    "                \n",
    "            test_score_adapt = 'Test on Adapt BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                    domain, epoch, test_score_adapt, best_valid_adapt)\n",
    "#             print('Adapt Results: ', test_score_adapt)\n",
    "#             wfile.write(test_score_adapt)\n",
    "            \n",
    "    print('Best on Regular BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "        domain, best_test_regular, best_valid_regular))\n",
    "    print('Best on InDomain BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "        domain, best_test_indomain, best_valid_indomain))\n",
    "    print('Best on Adapt BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "        domain, best_test_adapt, best_valid_adapt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaccine experiments\n",
    "vaccine_data = {\n",
    "    'docs': [],\n",
    "    'labels': [],\n",
    "}\n",
    "# wfile.write('\\nVaccine Evaluation---\\n')\n",
    "\n",
    "for idx, row in vaccine_df.iterrows():\n",
    "    encode_label = [0] * params['num_label']\n",
    "    for label_index, _ in enumerate(all_labels):\n",
    "        if np.isnan(np.array(row[all_labels[label_index]], dtype=np.float32)):\n",
    "            continue\n",
    "        if int(row[all_labels[label_index]]) == 1:\n",
    "            encode_label[label_index] = 1\n",
    "    if sum(encode_label) == 0:\n",
    "        encode_label[-1] = 1\n",
    "    vaccine_data['docs'].append(row['text'])\n",
    "    vaccine_data['labels'].append(encode_label)\n",
    "\n",
    "vaccine_train_docs, vaccine_test_docs, vaccine_train_labels, vaccine_test_labels = train_test_split(\n",
    "    vaccine_data['docs'], vaccine_data['labels'], test_size=.50)\n",
    "\n",
    "vaccine_train = {\n",
    "    'docs': [item for item in vaccine_train_docs],\n",
    "    'labels': [item for item in vaccine_train_labels],\n",
    "    'corpus': [1] * len(vaccine_train_docs)\n",
    "}\n",
    "vaccine_test = {\n",
    "    'docs': [item for item in vaccine_data['docs'][250:]],\n",
    "    'labels': [item for item in vaccine_data['labels'][250:]],\n",
    "    'corpus': [1] * len(vaccine_test_docs)\n",
    "}\n",
    "all_train = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': [0] * len(all_data.labels.to_list())\n",
    "}\n",
    "all_train['docs'].extend([item for item in vaccine_train['docs']])\n",
    "all_train['labels'].extend([item for item in vaccine_train['labels']])\n",
    "all_train['corpus'].extend([1] * len(vaccine_train['docs']))\n",
    "\n",
    "all_data_corpus = {\n",
    "    'docs': all_data.text.to_list(),\n",
    "    'labels': all_data.labels.to_list(),\n",
    "    'corpus': [0] * len(all_data.labels.to_list())\n",
    "}\n",
    "all_data_corpus['docs'].extend([item for item in vaccine_data['docs']])\n",
    "all_data_corpus['labels'].extend([item for item in vaccine_data['labels']])\n",
    "all_data_corpus['corpus'].extend([1] * 500)\n",
    "\n",
    "vaccine_train_data = TorchDataset(vaccine_train, domain_name=params['domain_name'])\n",
    "vaccine_test_data = TorchDataset(vaccine_test, domain_name=params['domain_name'])\n",
    "all_train_data = TorchDataset(all_train, domain_name=params['domain_name'])\n",
    "all_data_torch = TorchDataset(all_data_corpus, domain_name=params['domain_name'])\n",
    "\n",
    "vaccine_train_data_loader = DataLoader(\n",
    "    vaccine_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='bert')\n",
    ")\n",
    "train_data_loader = DataLoader(\n",
    "    all_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='bert')\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "    vaccine_train_data, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='bert')\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    vaccine_test_data, batch_size=params['batch_size'], shuffle=False,\n",
    "    collate_fn=DataEncoder(params, mtype='bert')\n",
    ")\n",
    "all_data_loader = DataLoader(\n",
    "    all_data_torch, batch_size=params['batch_size'], shuffle=True,\n",
    "    collate_fn=DataEncoder(params, mtype='bert')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust parameters\n",
    "params['epochs'] = 10\n",
    "params['lr'] = 9e-4\n",
    "params['emb_dim'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_bert.txt', 'model_dir': '../resource/model/adapt_bert/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 10, 'batch_size': 16, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'adam', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'bert_name': 'bert-base-uncased', 'num_label': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [05:30<49:34, 330.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Results:  Test on Adapt BERT, Domain vaccine, Epoch 0, F1-micro-average 0.6166666666666666, Valid Score 0.6004140786749482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [11:00<44:04, 330.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Results:  Test on Adapt BERT, Domain vaccine, Epoch 1, F1-micro-average 0.6085106382978723, Valid Score 0.6004140786749482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "omit_optim_names = ['bias', 'LayerNorm.weight']\n",
    "# regular_model = RegularBERT(params)\n",
    "# regular_model = regular_model.to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# regular_optim = torch.optim.Adam(\n",
    "#     [p for n, p in regular_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "#     lr=params['lr']\n",
    "# )\n",
    "\n",
    "# indomain_model = RegularBERT(params)\n",
    "# indomain_model = indomain_model.to(device)\n",
    "# indomain_optim = torch.optim.Adam(\n",
    "#     [p for n, p in indomain_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "#     lr=params['lr']\n",
    "# )\n",
    "\n",
    "adapt_model = AdaptBERT(params)\n",
    "adapt_model = adapt_model.to(device)\n",
    "domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "pred_params = [param for name, param in adapt_model.named_parameters() if 'domain' not in name and 'bert' not in name]\n",
    "adapt_pred_optim = torch.optim.Adam(pred_params, lr=params['lr'])\n",
    "domain_params = [param for name, param in adapt_model.named_parameters() if 'domain' in name]\n",
    "adapt_domain_optim = torch.optim.Adam(domain_params, lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "# best_valid_regular = 0.\n",
    "best_valid_adapt = 0.\n",
    "# best_valid_indomain = 0.\n",
    "\n",
    "# best_test_regular = 0.\n",
    "best_test_adapt = 0.\n",
    "# best_test_indomain = 0.\n",
    "\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "#     train_loss_regular = 0.\n",
    "    adapt_model.train()\n",
    "#     regular_model.train()\n",
    "#     indomain_model.train()\n",
    "    \n",
    "    # train indomain model for comparison\n",
    "#     for step, train_batch in enumerate(valid_data_loader):\n",
    "#         train_batch = tuple(t.to(device) for t in train_batch)\n",
    "#         input_docs, input_labels, input_domains = train_batch\n",
    "#         indomain_optim.zero_grad()\n",
    "#         # indomain models\n",
    "#         indomain_preds = indomain_model(**{'input_docs': input_docs})\n",
    "#         loss = criterion(indomain_preds, input_labels)\n",
    "#         loss.backward()\n",
    "#         indomain_optim.step()\n",
    "\n",
    "    # train discriminator first\n",
    "    for _ in range(3):\n",
    "        for step, train_batch in enumerate(all_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            adapt_domain_optim.zero_grad()\n",
    "            domain_preds = adapt_model.discriminator(**{'input_docs': input_docs})\n",
    "            domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "            domain_loss.backward()\n",
    "            adapt_domain_optim.step()\n",
    "\n",
    "    # train predictor\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        if len(input_docs) == 1:\n",
    "            continue\n",
    "#         regular_optim.zero_grad()\n",
    "        adapt_pred_optim.zero_grad()\n",
    "        # adapt_domain_optim.zero_grad()\n",
    "\n",
    "        # regular models\n",
    "#         regular_preds = regular_model(**{\n",
    "#             'input_docs': input_docs\n",
    "#         })\n",
    "#         loss = criterion(regular_preds, input_labels)\n",
    "#         train_loss_regular += loss.item()\n",
    "#         loss_avg_regular = train_loss_regular / (step + 1)\n",
    "\n",
    "        # adapt models\n",
    "        adapt_preds = adapt_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "        domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "        domain_preds_norm = domain_preds[:, 1]\n",
    "        domain_preds_norm = domain_preds_norm / torch.max(domain_preds_norm)\n",
    "        loss_adapt = loss_adapt.mean(axis=1)\n",
    "        loss_adapt = domain_preds_norm * loss_adapt\n",
    "        loss_adapt = loss_adapt.mean()\n",
    "\n",
    "        loss_adapt.backward()\n",
    "#         loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "#         regular_optim.step()\n",
    "        adapt_pred_optim.step()\n",
    "\n",
    "    # fit on in domain corpus.\n",
    "#     for _ in range(1):\n",
    "#         for step, train_batch in enumerate(valid_data_loader):\n",
    "#             train_batch = tuple(t.to(device) for t in train_batch)\n",
    "#             input_docs, input_labels, input_domains = train_batch\n",
    "#             if len(input_docs) == 1:\n",
    "#                 continue\n",
    "#             adapt_pred_optim.zero_grad()\n",
    "#             adapt_preds = adapt_model(**{'input_docs': input_docs})\n",
    "#             loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "#             loss_adapt = loss_adapt.mean()\n",
    "#             loss_adapt.backward()\n",
    "#             adapt_pred_optim.step()\n",
    "\n",
    "    # evaluate on valid data\n",
    "#     regular_model.eval()\n",
    "    adapt_model.eval()\n",
    "#     indomain_model.eval()\n",
    "#     y_preds_regular = []\n",
    "    y_preds_adapt = []\n",
    "#     y_preds_indomain = []\n",
    "    y_trues = []\n",
    "\n",
    "    for valid_batch in valid_data_loader:\n",
    "        valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "        input_docs, input_labels, input_domains = valid_batch\n",
    "        with torch.no_grad():\n",
    "#             preds_regular = regular_model(**{'input_docs': input_docs})\n",
    "            preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "#             preds_indomain = indomain_model(**{'input_docs': input_docs})\n",
    "\n",
    "#         logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "        logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "#         logits_indomain = (torch.sigmoid(preds_indomain) > .4).long().cpu().numpy()\n",
    "\n",
    "#         y_preds_regular.extend(logits_regular)\n",
    "        y_preds_adapt.extend(logits_adapt)\n",
    "#         y_preds_indomain.extend(logits_indomain)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "#     eval_score_regular = micro_f1_average(y_preds=y_preds_regular, y_truths=y_trues)\n",
    "    eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "#     eval_score_indomain = micro_f1_average(y_preds=y_preds_indomain, y_truths=y_trues)\n",
    "\n",
    "    # test for regular model\n",
    "#     if eval_score_regular > best_valid_regular:\n",
    "#         best_valid_regular = eval_score_regular\n",
    "\n",
    "#         # test\n",
    "#         y_preds = []\n",
    "#         y_trues = []\n",
    "#         # evaluate on the test set\n",
    "#         for test_batch in test_data_loader:\n",
    "#             test_batch = tuple(t.to(device) for t in test_batch)\n",
    "#             input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 preds_regular = regular_model(**{\n",
    "#                     'input_docs': input_docs,\n",
    "#                 })\n",
    "#             logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "#             y_preds.extend(logits_regular)\n",
    "#             y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "#         test_score_regular = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "#         if best_test_regular < test_score_regular:\n",
    "#             best_test_regular = test_score_regular\n",
    "#             torch.save(regular_model, params['model_dir'] + 'regular_bert_vaccine.pth')\n",
    "            \n",
    "#         regular_results = 'Test on Regular BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "#                 'vaccine', epoch, test_score_regular, best_valid_regular)\n",
    "#         print('Regular Results: ', regular_results)\n",
    "#         wfile.write(regular_results)\n",
    "\n",
    "    # test for indomain model\n",
    "#     if eval_score_indomain > best_valid_indomain:\n",
    "#         best_valid_indomain = eval_score_indomain\n",
    "\n",
    "#         # test\n",
    "#         y_preds = []\n",
    "#         y_trues = []\n",
    "#         # evaluate on the test set\n",
    "#         for test_batch in test_data_loader:\n",
    "#             test_batch = tuple(t.to(device) for t in test_batch)\n",
    "#             input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 preds_indomain = indomain_model(**{\n",
    "#                     'input_docs': input_docs,\n",
    "#                 })\n",
    "#             logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "#             y_preds.extend(logits_indomain)\n",
    "#             y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "#         test_score_indomain = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "#         if best_test_indomain < test_score_indomain:\n",
    "#             best_test_indomain = test_score_indomain\n",
    "#             torch.save(indomain_model, params['model_dir'] + 'indomain_bert_vaccine.pth')\n",
    "            \n",
    "#         indomain_results = 'Test on Indomain BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "#                 'vaccine', epoch, test_score_indomain, best_valid_indomain)\n",
    "#         print('Regular Results: ', indomain_results)\n",
    "#         wfile.write(indomain_results)\n",
    "\n",
    "    if eval_score_adapt > best_valid_adapt:\n",
    "        best_valid_adapt = eval_score_adapt\n",
    "\n",
    "    # test\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    # evaluate on the test set\n",
    "    for test_batch in test_data_loader:\n",
    "        test_batch = tuple(t.to(device) for t in test_batch)\n",
    "        input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_adapt = adapt_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "        logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "        y_preds.extend(logits_adapt)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "    test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "    if test_score_adapt > best_test_adapt:\n",
    "        best_test_adapt = test_score_adapt\n",
    "        torch.save(adapt_model, params['model_dir'] + 'adapt_bert_vaccine.pth')\n",
    "    test_score_adapt = 'Test on Adapt BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            'vaccine', epoch, test_score_adapt, best_valid_adapt)\n",
    "    print('Adapt Results: ', test_score_adapt)\n",
    "#         wfile.write(test_score_adapt)\n",
    "\n",
    "# wfile.write('\\n\\n\\n')\n",
    "# wfile.close()\n",
    "\n",
    "# print('Best on Regular BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "#             domain, best_test_regular, best_valid_regular))\n",
    "# print('Best on InDomain BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "#             domain, best_test_indomain, best_valid_indomain))\n",
    "print('Best on Adapt BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_adapt, best_valid_adapt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train...\n",
      "{'result_path': '../resource/results/adapt_bert.txt', 'model_dir': '../resource/model/adapt_bert/', 'dname': 'all', 'dpath': '../data/dataset.tsv', 'max_feature': 15000, 'over_sample': True, 'domain_name': 'corpus', 'epochs': 10, 'batch_size': 64, 'lr': 9e-05, 'max_len': 60, 'dp_rate': 0.2, 'optimizer': 'adam', 'emb_dim': 200, 'unique_domains': ['ALM', 'Baltimore', 'BLM', 'Davidson', 'Election', 'MeToo', 'Sandy'], 'bidirectional': False, 'device': device(type='cuda'), 'bert_name': 'bert-base-uncased', 'num_label': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [37:38<00:00, 225.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on Regular RNN, Domain Sandy, F1-micro-average 0.7905074592477391, Valid Score 0.9978333054952397\n",
      "\n",
      "Best on InDomain RNN, Domain Sandy, F1-micro-average 0.625988787343522, Valid Score 0.7001587826129008\n",
      "\n",
      "Best on Adapt RNN, Domain Sandy, F1-micro-average 0.584, Valid Score 0.6252505010020041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "omit_optim_names = ['bias', 'LayerNorm.weight']\n",
    "regular_model = RegularBERT(params)\n",
    "regular_model = regular_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "domain_criterion = nn.CrossEntropyLoss().to(device)\n",
    "regular_optim = torch.optim.Adam(\n",
    "    [p for n, p in regular_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "    lr=params['lr']\n",
    ")\n",
    "\n",
    "indomain_model = RegularBERT(params)\n",
    "indomain_model = indomain_model.to(device)\n",
    "indomain_optim = torch.optim.Adam(\n",
    "    [p for n, p in indomain_model.named_parameters() if not any(nd in n for nd in omit_optim_names)],\n",
    "    lr=params['lr']\n",
    ")\n",
    "\n",
    "adapt_model = AdaptBERT(params)\n",
    "adapt_model = adapt_model.to(device)\n",
    "criterion_adapt = nn.BCEWithLogitsLoss(reduction='none').to(device)\n",
    "pred_params = [param for name, param in adapt_model.named_parameters() if 'domain' not in name and not any(nd in name for nd in omit_optim_names)]\n",
    "adapt_pred_optim = torch.optim.Adam(pred_params, lr=params['lr'])\n",
    "domain_params = [param for name, param in adapt_model.named_parameters() if 'domain' in name and not any(nd in name for nd in omit_optim_names)]\n",
    "adapt_domain_optim = torch.optim.Adam(domain_params, lr=params['lr'])\n",
    "\n",
    "# train the networks\n",
    "print('Start to train...')\n",
    "print(params)\n",
    "best_valid_regular = 0.\n",
    "best_valid_adapt = 0.\n",
    "best_valid_indomain = 0.\n",
    "\n",
    "best_test_regular = 0.\n",
    "best_test_adapt = 0.\n",
    "best_test_indomain = 0.\n",
    "\n",
    "for epoch in tqdm(range(params['epochs'])):\n",
    "    train_loss_regular = 0.\n",
    "    train_loss_adapt = 0.\n",
    "    adapt_model.train()\n",
    "    regular_model.train()\n",
    "    indomain_model.train()\n",
    "    \n",
    "    # train indomain model for comparison\n",
    "    for step, train_batch in enumerate(valid_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        indomain_optim.zero_grad()\n",
    "        # indomain models\n",
    "        indomain_preds = indomain_model(**{'input_docs': input_docs})\n",
    "        loss = criterion(indomain_preds, input_labels)\n",
    "        loss.backward()\n",
    "        indomain_optim.step()\n",
    "\n",
    "    # train discriminator first\n",
    "    for step, train_batch in enumerate(all_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        adapt_domain_optim.zero_grad()\n",
    "        domain_preds = adapt_model.discriminator(**{'input_docs': input_docs})\n",
    "        domain_loss = domain_criterion(domain_preds, input_domains)\n",
    "        domain_loss.backward()\n",
    "        adapt_domain_optim.step()\n",
    "\n",
    "    # train predictor\n",
    "    for step, train_batch in enumerate(train_data_loader):\n",
    "        train_batch = tuple(t.to(device) for t in train_batch)\n",
    "        input_docs, input_labels, input_domains = train_batch\n",
    "        if len(input_docs) == 1:\n",
    "            continue\n",
    "        regular_optim.zero_grad()\n",
    "        adapt_pred_optim.zero_grad()\n",
    "        # adapt_domain_optim.zero_grad()\n",
    "\n",
    "        # regular models\n",
    "        regular_preds = regular_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss = criterion(regular_preds, input_labels)\n",
    "        train_loss_regular += loss.item()\n",
    "        loss_avg_regular = train_loss_regular / (step + 1)\n",
    "\n",
    "        # adapt models\n",
    "        adapt_preds = adapt_model(**{\n",
    "            'input_docs': input_docs\n",
    "        })\n",
    "        loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "        domain_preds = torch.sigmoid(adapt_model.discriminator(**{'input_docs': input_docs}))\n",
    "        loss_adapt = loss_adapt.mean(axis=1)\n",
    "        loss_adapt = domain_preds[:, 1] * loss_adapt\n",
    "        loss_adapt = loss_adapt.mean()\n",
    "        train_loss_adapt += loss_adapt.item()\n",
    "        loss_avg_adapt = train_loss_adapt / (step + 1)\n",
    "\n",
    "        loss_adapt.backward()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 0.5)\n",
    "        regular_optim.step()\n",
    "        adapt_pred_optim.step()\n",
    "\n",
    "    # fit on in domain corpus.\n",
    "    for _ in range(5):\n",
    "        for step, train_batch in enumerate(valid_data_loader):\n",
    "            train_batch = tuple(t.to(device) for t in train_batch)\n",
    "            input_docs, input_labels, input_domains = train_batch\n",
    "            if len(input_docs) == 1:\n",
    "                continue\n",
    "            adapt_pred_optim.zero_grad()\n",
    "            adapt_preds = adapt_model(**{'input_docs': input_docs})\n",
    "            loss_adapt = criterion_adapt(adapt_preds, input_labels)\n",
    "            loss_adapt = loss_adapt.mean()\n",
    "            loss_adapt.backward()\n",
    "            adapt_pred_optim.step()\n",
    "\n",
    "    # evaluate on valid data\n",
    "    regular_model.eval()\n",
    "    adapt_model.eval()\n",
    "    indomain_model.eval()\n",
    "    y_preds_regular = []\n",
    "    y_preds_adapt = []\n",
    "    y_preds_indomain = []\n",
    "    y_trues = []\n",
    "\n",
    "    for valid_batch in valid_data_loader:\n",
    "        valid_batch = tuple(t.to(device) for t in valid_batch)\n",
    "        input_docs, input_labels, input_domains = valid_batch\n",
    "        with torch.no_grad():\n",
    "            preds_regular = regular_model(**{'input_docs': input_docs})\n",
    "            preds_adapt = adapt_model(**{'input_docs': input_docs})\n",
    "            preds_indomain = indomain_model(**{'input_docs': input_docs})\n",
    "\n",
    "        logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "        logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "        logits_indomain = (torch.sigmoid(preds_indomain) > .4).long().cpu().numpy()\n",
    "\n",
    "        y_preds_regular.extend(logits_regular)\n",
    "        y_preds_adapt.extend(logits_adapt)\n",
    "        y_preds_indomain.extend(logits_indomain)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "    eval_score_regular = micro_f1_average(y_preds=y_preds_regular, y_truths=y_trues)\n",
    "    eval_score_adapt = micro_f1_average(y_preds=y_preds_adapt, y_truths=y_trues)\n",
    "    eval_score_indomain = micro_f1_average(y_preds=y_preds_indomain, y_truths=y_trues)\n",
    "\n",
    "    # test for regular model\n",
    "    if eval_score_regular > best_valid_regular:\n",
    "        best_valid_regular = eval_score_regular\n",
    "\n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_regular = regular_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_regular = (torch.sigmoid(preds_regular) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_regular)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_regular = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if best_test_regular < test_score_regular:\n",
    "            best_test_regular = test_score_regular\n",
    "            torch.save(regular_model, params['model_dir'] + 'regular_bert_vaccine.pth')\n",
    "            \n",
    "        regular_results = 'Test on Regular BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                'vaccine', epoch, test_score_regular, best_valid_regular)\n",
    "#         print('Regular Results: ', regular_results)\n",
    "#         wfile.write(regular_results)\n",
    "\n",
    "    # test for indomain model\n",
    "    if eval_score_indomain > best_valid_indomain:\n",
    "        best_valid_indomain = eval_score_indomain\n",
    "\n",
    "        # test\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        # evaluate on the test set\n",
    "        for test_batch in test_data_loader:\n",
    "            test_batch = tuple(t.to(device) for t in test_batch)\n",
    "            input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds_indomain = indomain_model(**{\n",
    "                    'input_docs': input_docs,\n",
    "                })\n",
    "            logits_indomain = (torch.sigmoid(preds_indomain) > .5).long().cpu().numpy()\n",
    "            y_preds.extend(logits_indomain)\n",
    "            y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "        test_score_indomain = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "        if best_test_indomain < test_score_indomain:\n",
    "            best_test_indomain = test_score_indomain\n",
    "            torch.save(indomain_model, params['model_dir'] + 'indomain_bert_vaccine.pth')\n",
    "            \n",
    "        indomain_results = 'Test on Indomain BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "                'vaccine', epoch, test_score_indomain, best_valid_indomain)\n",
    "#         print('Regular Results: ', indomain_results)\n",
    "#         wfile.write(indomain_results)\n",
    "\n",
    "    if eval_score_adapt > best_valid_adapt:\n",
    "        best_valid_adapt = eval_score_adapt\n",
    "\n",
    "    # test\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    # evaluate on the test set\n",
    "    for test_batch in test_data_loader:\n",
    "        test_batch = tuple(t.to(device) for t in test_batch)\n",
    "        input_docs, input_labels, input_domains = test_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_adapt = adapt_model(**{\n",
    "                'input_docs': input_docs,\n",
    "            })\n",
    "        logits_adapt = (torch.sigmoid(preds_adapt) > .5).long().cpu().numpy()\n",
    "        y_preds.extend(logits_adapt)\n",
    "        y_trues.extend(input_labels.to('cpu').numpy())\n",
    "\n",
    "    test_score_adapt = micro_f1_average(y_preds=y_preds, y_truths=y_trues)\n",
    "    if test_score_adapt > best_test_adapt:\n",
    "        best_test_adapt = test_score_adapt\n",
    "        torch.save(adapt_model, params['model_dir'] + 'adapt_bert_vaccine.pth')\n",
    "    test_score_adapt = 'Test on Adapt BERT, Domain {}, Epoch {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            'vaccine', epoch, test_score_adapt, best_valid_adapt)\n",
    "#         print('Adapt Results: ', test_score_adapt)\n",
    "#         wfile.write(test_score_adapt)\n",
    "\n",
    "# wfile.write('\\n\\n\\n')\n",
    "# wfile.close()\n",
    "\n",
    "print('Best on Regular BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_regular, best_valid_regular))\n",
    "print('Best on InDomain BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_indomain, best_valid_indomain))\n",
    "print('Best on Adapt BERT, Domain {}, F1-micro-average {}, Valid Score {}\\n'.format(\n",
    "            domain, best_test_adapt, best_valid_adapt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
